%% 
%% Copyright 2019-2021 Elsevier Ltd
%% 
%% This file is part of the 'CAS Bundle'.
%% --------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'CAS Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for cas-sc documentclass for 
%% single column output.

\documentclass[a4paper,fleqn]{cas-sc}

% If the frontmatter runs over more than one page
% use the longmktitle option.

%\documentclass[a4paper,fleqn,longmktitle]{cas-sc}

%\usepackage[numbers]{natbib}
%\usepackage[authoryear]{natbib}
\usepackage[authoryear,longnamesfirst]{natbib}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{graphicx}
%\usepackage{subfig}


%%%Author macros
\def\tsc#1{\csdef{#1}{\textsc{\lowercase{#1}}\xspace}}
\tsc{WGM}
\tsc{QE}
%%%

% Uncomment and use as if needed
%\newtheorem{theorem}{Theorem}
%\newtheorem{lemma}[theorem]{Lemma}
%\newdefinition{rmk}{Remark}
%\newproof{pf}{Proof}
%\newproof{pot}{Proof of Theorem \ref{thm}}

\begin{document}
\let\WriteBookmarks\relax
\def\floatpagepagefraction{1}
\def\textpagefraction{.001}

% Short title
\shorttitle{Multi-Modal-Multi-Task Classification}

% Short author
%\shortauthors{G Manoranjan et~al.}

% Main title of the paper
\title[mode = title]{Ultra-Edge Multimodal Knowledge Distillation for Medical Image Classification: A Two-Phase Optimization Approach with Multi-Head Cross-Attention Fusion}                      
% Title footnote mark
% eg: \tnotemark[1]
%\tnotemark[1,2]

% Title footnote 1.
% eg: \tnotetext[1]{Title footnote text}
% \tnotetext[<tnote number>]{<tnote text>} 


% First author
%
% Options: Use if required
% eg: \author[1,3]{Author Name}[type=editor,
%       style=chinese,
%       auid=000,
%       bioid=1,
%       prefix=Sir,
%       orcid=0000-0000-0000-0000,
%       facebook=<facebook id>,
%       twitter=<twitter id>,
%       linkedin=<linkedin id>,
%       gplus=<gplus id>]

%\author[1]{Manoranjan Gandhudi}[orcid=0000-0002-5886-830X]

% Corresponding author indication
%\cormark[1]

% Footnote of the first author
%\fnmark[1]

% Email id of the first author
%\ead{manogr@nitt.edu}

% URL of the first author
%\ead[url]{www.cvr.cc, cvr@sayahna.org}

%  Credit authorship
%\credit{Conceptualization of this study, Methodology, Software}

% Address/affiliation
%\affiliation[1]{organization={National Institute of Technology Tiruchirappalli},
   % addressline={Department of Computer Applications}, 
   % city={Trichy},
    % citysep={}, % Uncomment if no comma needed between city and postcode
    %postcode={620015}, 
    % state={},
%    country={India}}

% Second author


% Third author
%\author[1]{Alphonse PJA}[orcid=0000-0003-2899-5911]
%\fnmark[2]
%\ead{alphonse@nitt.edu}
%\ead[URL]{www.sayahna.org}

%\credit{Data curation, Writing - Original draft preparation}


% Fourth author

%\cormark[2]
%\fnmark[1,3]
%\ead{rishi@stmdocs.in}
%\ead[URL]{www.stmdocs.in}

%\author[2]{Ugo Fiore}[orcid=0000-0003-0509-5662]

%\author[2]{Leeladhar Nagineni}
%\author[1]{Gangadharan GR}[orcid=0000-0002-0764-2650]
%\ead{ganga@nitt.edu}
%\affiliation[2]{organization={University of Salerno},
    %addressline={Mepukada}, 
  %  city={Fisciano},
    % citysep={}, % Uncomment if no comma needed between city and postcode
    %postcode={695571}, 
    %state={Trivandrum},
  %  country={Italy}}

% Corresponding author text
%\cortext[cor1]{Corresponding author}


% Footnote text


% Here goes the abstract
\begin{abstract}
Medical image analysis demands efficient neural networks deployable on edge devices without sacrificing diagnostic accuracy. This paper presents a novel ultra-edge multimodal knowledge distillation framework that combines vision and language modalities through multi-head cross-attention fusion. We employ a two-phase knowledge distillation approach: Phase 1 trains a large teacher model (ViT-Base + BioClinical-BERT) for 3 epochs with standard cross-entropy loss; Phase 2 trains a lightweight student model (MobileViT-Small + BERT-Mini) for 10 epochs using combined knowledge distillation loss. Crucially, we apply automated hyperparameter optimization using Optuna with Tree-structured Parzen Estimator (TPE) sampling to discover optimal training configurations. Our best configuration achieves a validation F1 score of 92.94\%, representing a 6-9\% improvement over baseline configurations. Evaluation on two medical datasets (MedPix-2.0 and Wound-1.0) demonstrates consistent improvements across 11 different student architectures. The proposed student model (MobileViT-small-bert-mini) achieves 90.8\% average F1 score while maintaining only 17.13 million parameters and sub-5ms inference latency, making it suitable for deployment on mobile and edge devices.

\end{abstract}

% Use if graphical abstract is present
% \begin{graphicalabstract}
% \includegraphics{figs/grabs.pdf}
% \end{graphicalabstract}

% Research highlights
\begin{highlights}
\item I

\item C

\item T
\end{highlights}

% Keywords
% Each keyword is seperated by \sep
\begin{keywords}
E
\end{keywords}

%, , , , .

\maketitle



\section{Introduction}
Medical image analysis has become a cornerstone of modern healthcare, enabling precise diagnostics, treatment planning, and patient monitoring \cite{maier2019medical}. With advancements in artificial intelligence (AI), particularly deep learning, the field has witnessed significant progress in automating tasks such as image segmentation, classification, and report generation. However, traditional unimodal models, which process either images or text in isolation, often fail to capture the interconnected nature of clinical data, limiting their diagnostic accuracy. Multimodal AI addresses this by integrating diverse data sources, including medical images (e.g., CT and MRI scans), clinical notes, genomic data, and sensor readings from wearables, leading to more comprehensive and accurate insights \cite{acosta2022multimodal}.

Large vision-language models (VLMs) like CLIP \cite{radford2021learning} have demonstrated superior capabilities in aligning visual and textual representations, outperforming prior models in tasks such as zero-shot classification and visual question answering due to their robust pre-training on diverse datasets \cite{cherti2023reproducible}. The Table \ref{tab:VLM_lightweight} provides an overview of popular lightweight vision-language models (VLMs), their parameter sizes, and capabilities, making it a practical reference for selecting models suitable for resource-limited or edge-device scenarios.

In healthcare, these models show promise for automating radiology report generation, disease diagnosis, and personalized treatment recommendations \cite{moor2023med}. However, their deployment is hindered by high computational demands, including large parameter counts (often exceeding 1 billion) and significant inference latency, which are impractical for resource-constrained environments such as mobile devices, rural clinics, or edge computing setups in telemedicine \cite{huang2022multimodal}. Edge devices in healthcare typically operate under strict power and memory constraints, necessitating models that balance accuracy with efficiency.

Despite significant advancements in VLM models, several research gaps persist. First, while large VLMs achieve high performance, their complexity makes them unsuitable for real-time medical applications on low-resource hardware, particularly in underserved regions where computational infrastructure is limited \cite{huang2022multimodal}. Second, most existing lightweight models focus on unimodal tasks (e.g., image-only classification), lacking the ability to leverage multimodal clinical data, which is critical for holistic diagnostics \cite{zhang2024vision}. Third, knowledge distillation (KD) has been applied to compress vision or language models separately \cite{gou2021knowledge, jiao2019tinybert}, but its application to multimodal medical VLMs remains underexplored, especially for datasets with structured clinical text like MedPix 2.0 \cite{abdullah2024medpix}. Finally, the integration of advanced optimization techniques, such as label smoothing, mixed-precision training, gradient accumulation, progressive layer unfreezing, pruning and quantization, with KD for medical VLMs is rarely addressed, leaving a gap in deployable solutions for edge-based healthcare AI.

To address these gaps, knowledge distillation \cite{hinton2015distilling} is employed to transfer knowledge from a large, complex teacher model to a smaller student model, preserving performance while reducing size and computational overhead. This is particularly relevant in medical AI, where data privacy concerns limit access to large-scale datasets, and model efficiency enables real-time applications like point-of-care diagnostics.

This work presents a lightweight VLM designed for classifying medical image modalities (CT vs. MRI) and anatomical locations using the MedPix 2.0 dataset \cite{abdullah2024medpix}, a curated multimodal biomedical dataset with high-quality images (CT, MRI) from 671 cases, paired with structured clinical texts. By distilling knowledge from a teacher model combining CLIP’s vision encoder \cite{radford2021learning} and DistilBERT’s text encoder \cite{sanh2019distilbert}, our student model achieves efficient multimodal classification. The approach integrates advanced optimization strategies, including knowledge distillation with label smoothing, mixed-precision training, gradient accumulation, progressive layer unfreezing, pruning, and dynamic quantization, to ensure deployability in low-resource settings. Specifically, the key contributions of this paper are:
\begin{enumerate}
    \item A compact Lightweight VLM architecture, featuring a lightweight CNN for vision and LSTM for text for efficient multimodal medical models.
    \item An optimized training pipeline integrating knowledge distillation with label smoothing, mixed-precision train-
ing, gradient accumulation, progressive layer unfreezing, pruning and dynamic quantization, tailored for medical edge applications. 
    \item A detailed evaluation on the MedPix 2.0 validation split, demonstrating convergence to near-perfect accuracy in modality and location classification, filling the gap in high-performance lightweight VLMs.
    \item An open-source implementation to facilitate reproducibility and further research in efficient medical AI, contributing to the accessibility of advanced healthcare solutions.
\end{enumerate}

The rest of the paper is organized as follows. Section \ref{sec:related-work} reviews related work in knowledge distillation, multimodal medical AI, and model compression. Section \ref{sec:proposed-method} details the proposed lightweight VLM architecture, dataset, training, and optimization methods. This is followed by Section \ref{sec:results} on performance, evaluation and results, Section \ref{sec:discussion} on discussion, and Section \ref{sec:conclusion} on the conclusion.

\section{Related Work}\label{sec2}
The development of our lightweight vision-language model (VLM) builds on progress in knowledge distillation, multimodal learning for medical applications, and model compression techniques. Here, we survey key contributions in these areas, highlighting their relevance and identifying gaps that our work addresses.

\subsection{Knowledge Distillation}
Knowledge distillation (KD) transfers expertise from large, high-performing teacher models to compact student models, balancing efficiency and accuracy \cite{hinton2015distilling}. In computer vision, KD has been widely applied to compress convolutional networks \cite{gou2021knowledge}, while in natural language processing, it has produced efficient models like DistilBERT \cite{sanh2019distilbert} and TinyBERT \cite{jiao2019tinybert}. Recent efforts extend KD to multimodal settings, such as DistilVLM, which aligns student vision-language representations with teacher outputs for general tasks \cite{gu2021distilling}. In medical imaging, KD has shown promise for tasks like tumor segmentation and disease classification \cite{ma2024knowledge, li2024medalmighty}. For instance, \cite{he2024task}  uses task-specific KD with Low-Rank Adaptation (LoRA) to distill vision foundation models, while \cite{he2024m3ae} introduces M3AE-Distill for medical vision-language pretraining, supporting classification and segmentation.

Despite these advances, applying KD to multimodal medical VLMs remains underexplored, particularly for datasets like MedPix 2.0, which combines structured clinical text with images \cite{abdullah2024medpix}. Most KD studies focus on unimodal tasks or general-domain VLMs, leaving a gap in lightweight, domain-specific multimodal models for healthcare.

\subsection{Multimodal Learning in Healthcare}
Multimodal AI integrates diverse data sources, such as medical images and clinical reports, to enhance diagnostic accuracy \cite{acosta2022multimodal}. Transformer-based models and graph neural networks have been used to fuse imaging and text for tasks like report generation and visual question answering (VQA) \cite{huang2022multimodal, tayebi2024multimodal}. Datasets like MIMIC-CXR \cite{johnson2019mimic} and VQA-RAD \cite{lau2018dataset} provide paired image-text data, but they often lack the structured annotations and balanced modality distributions found in MedPix 2.0 \cite{abdullah2024medpix}.

Large VLMs, such as BiomedCLIP \cite{moor2023foundation}, excel in medical tasks like zero-shot classification, but their size limits deployment in resource-constrained settings. Efforts to create lightweight medical VLMs include \cite{wang2024lightweight}, which combines BiomedCLIP with LLaMA-3 for multimodal VQA, and \cite{li2024mixture}, which uses a mixture-of-experts approach for parameter efficiency. However, these models often prioritize single tasks or require extensive pretraining, and few address the specific challenge of joint modality and location classification on curated datasets like MedPix 2.0.

\subsection{Model Compression}
Model compression techniques, such as pruning \cite{han2015learning} and quantization \cite{krishnamoorthi2018quantizing}, reduce computational overhead, making models viable for edge devices. In medical AI, compression is critical for enabling real-time diagnostics in low-resource environments. While works apply pruning to unimodal medical image classifiers, integrating compression with KD for multimodal VLMs is less common. This gap is significant, as multimodal models require careful optimization to maintain performance across vision and text tasks.

Our work addresses these gaps by developing a lightweight VLM for modality and location classification on MedPix 2.0, combining KD with pruning and quantization. Unlike prior approaches, we focus on a compact, domain-specific model optimized for edge deployment, leveraging the structured annotations of MedPix 2.0 to achieve high performance with minimal resources.

\subsection{Problem Formulation}
The primary objective of this work is to develop a lightweight multimodal vision-language model suitable for deployment on resource-constrained edge devices, capable of performing multi-task medical image classification through an efficient two-phase knowledge distillation process.
In the first phase, a large teacher model---comprising ViT-Base as the vision encoder and BioClinical-BERT as the text encoder---is trained for a limited number of epochs (e.g., 3) on multimodal datasets including MedPix-2.0 and Wound-1.0, optimized solely via standard cross-entropy loss. In the second phase, a compact student model---employing MobileViT-Small as the vision encoder and BERT-Mini as the text encoder---is trained for additional epochs (e.g., 10) using the frozen teacher's logits and intermediate features as distillation targets.
Both the teacher and student models incorporate an identical multi-head cross-attention fusion mechanism to integrate vision and text representations, producing a unified multimodal embedding that feeds into two independent classification heads: one for image modality (e.g., CT versus MRI, or wound type) and one for anatomical location or severity.
Formally, the student model $S$ is optimized to approximate the behavior of the teacher model $T$ across both tasks while adhering to strict constraints on parameter count (fewer than 20 million) and inference latency (less than 5 ms), ensuring hardware-aware efficiency for edge deployment. This is achieved by minimizing the composite objective
$$\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{CE}} + \alpha \mathcal{L}_{\text{KD}} + \beta \mathcal{L}_{\text{MSE}} + \gamma \mathcal{L}_{\text{CRD}},$$
where $\mathcal{L}_{\text{CE}}$ denotes the supervised cross-entropy loss on ground-truth labels, $\mathcal{L}_{\text{KD}}$ the temperature-scaled Kullback--Leibler divergence between teacher and student softmax distributions, $\mathcal{L}_{\text{MSE}}$ the mean squared error on projected intermediate features from both modalities, and $\mathcal{L}_{\text{CRD}}$ the contrastive representation distillation loss enforcing instance-level alignment. The hyperparameters $\alpha$, $\beta$, $\gamma$, learning rates, distillation temperature $T$, and fusion embedding dimension are systematically tuned using Optuna with a Tree-structured Parzen Estimator sampler over 30 trials, selecting the configuration that maximizes the average macro-F1 score on the validation sets.
This decoupled two-phase formulation ensures effective transfer of multimodal knowledge without parameter sharing between teacher and student, yielding high classification performance while maintaining suitability for real-time edge deployment in medical applications.


\section{Problem Formulation}

We study multimodal medical classification under strict edge resource limits and latency bounds.

Let each sample be a pair $(x_i^{img}, x_i^{txt})$ with labels $(y_i^{a}, y_i^{b})$.

Here, $y_i^{a}$ denotes image modality and $y_i^{b}$ denotes anatomical or clinical class.

The dataset is defined as $\mathcal{D}=\{(x_i^{img},x_i^{txt},y_i^{a},y_i^{b})\}_{i=1}^{N}$.

A large teacher model $f_t$ maps inputs to logits $(z_t^{a}, z_t^{b})$.

A compact student model $f_s$ produces logits $(z_s^{a}, z_s^{b})$ under tight constraints.

The goal is to train $f_s$ to match teacher behavior with minimal accuracy loss.

Student predictions are given by softmax probabilities $p_s^{k}=\text{softmax}(z_s^{k})$.

Teacher soft targets use temperature scaling $T>1$ as $p_t^{k}(T)=\text{softmax}(z_t^{k}/T)$.

We define supervised loss using cross entropy on ground truth labels.

\begin{equation}
\mathcal{L}_{CE} = -\log p_s^{a}(y^{a}) - \log p_s^{b}(y^{b})
\end{equation}

Knowledge transfer is enforced using KL divergence between teacher and student outputs.

\begin{equation}
\mathcal{L}_{KD} = T^2 \sum_{k\in\{a,b\}} KL(p_t^{k}(T)\Vert p_s^{k}(T))
\end{equation}

To align representations, feature regression loss matches projected teacher features.

\begin{equation}
\mathcal{L}_{FR} = \lVert h_s - W h_t \rVert_2^2
\end{equation}

The final optimization objective combines all losses with fixed weights.

\begin{equation}
\mathcal{L}_{total} = \mathcal{L}_{CE} + \alpha \mathcal{L}_{KD} + \beta \mathcal{L}_{FR}
\end{equation}

This formulation targets accurate multimodal prediction with efficient edge deployment.




\section{Proposed Methodology}\label{sec3}
Our methodology introduces a unified framework that distills multimodal knowledge from a large teacher into an edge-ready student model. The design focuses on preserving diagnostic performance while reducing model size and inference cost. It consists of five components: Phase 1(teacher pretraining), Phase 2(student distillation), cross-attention fusion, loss function formulation, and hyperparameter optimization. Each component is described in detail in the following subsections.
Our framework Fig. \ref{fig:system-arch} explicitly decouples knowledge distillation into two distinct phases, avoiding feature-sharing between teacher and student networks:

\begin{figure}
    \centering
    \includegraphics[width=14cm, height = 8cm]{results/system_architecture.png}
    \caption{System Architecture of Proposed Pipeline}
    \label{fig:system-arch}
\end{figure}

\subsection{Phase 1 - Teacher Pretraining :} The teacher model, composed of ViT-Base as the vision encoder and BioClinical-BERT as the text encoder, is trained on combined datasets with standard cross-entropy loss \ref{eq1} \ref{eq2} \ref{eq3}.

\subsection{Phase 2 - Student distillation :} The student model (MobileViT-xx-Small + BERT-Mini) is trained using a combined loss function detaile in \ref{loss-details}

The knowledge distillation loss is computed using soft targets from the teacher.
\subsection{Combined Knowledge Distillation Loss} \label{loss-details}

The proposed training objective for the student network is a weighted combination of four complementary terms: supervised cross-entropy on clinical labels, logit-based knowledge distillation, feature-level regression, and contrastive representation distillation. 
% Formally, the total loss is
% \begin{equation}
% L_{\text{total}}
% = L_{\text{CE}}
% + \alpha\, L_{\text{KD}}
% + \beta\, L_{\text{MSE}}
% + \gamma\, L_{\text{CRD}},
% \end{equation}
% where $\alpha$, $\beta$, and $\gamma$ are non-negative scalars that control the relative contribution of each distillation component, and the temperature $T$ is used in the softmax-based KD term.\,[web:1]

\subsubsection*{Supervised Cross-Entropy Loss}
For each input sample, the student outputs logits for two classification tasks: image modality and anatomical location. Let $z^{\text{mod}}_{s} \in \mathbb{R}^{C_{\text{mod}}}$ and $z^{\text{loc}}_{s} \in \mathbb{R}^{C_{\text{loc}}}$ denote the student logits for modality and location, and let $y_{\text{mod}}$ and $y_{\text{loc}}$ be the corresponding ground-truth labels. The softmax probabilities for a task with logits $z \in \mathbb{R}^{C}$ are

\begin{equation}
p_c = \frac{\exp(z_c)}{\sum_{j=1}^{C} \exp(z_j)}.
\end{equation}\label{eq1}
The cross-entropy loss for a single task is then
\begin{equation}
L_{\text{CE}}(z, y) = -\log p_y.
\end{equation}\label{eq2}
and the total supervised term over the two tasks becomes
\begin{equation}
L_{\text{CE}}
= L_{\text{CE}}^{\text{mod}}
+ L_{\text{CE}}^{\text{loc}}
= -\log p^{\text{mod}}_{y_{\text{mod}}}
  -\log p^{\text{loc}}_{y_{\text{loc}}}.
\end{equation}\label{eq3}
This term anchors the student to the ground-truth labels for both modality and location classification.\,[web:1]

\subsubsection*{Logit-Based Knowledge Distillation Loss}
To transfer the teacher's class-conditional structure, we use a KL-divergence loss between temperature-scaled teacher and student distributions for both tasks.\,[web:1] Let $z^{\text{mod}}_{t}$ and $z^{\text{loc}}_{t}$ denote the teacher logits. For a generic task with teacher logits $z_t$ and student logits $z_s$, the softened probability distributions at temperature $T$ are
\begin{equation}
p^{(T)}_t(c) = \frac{\exp(z_{t,c} / T)}{\sum_{j} \exp(z_{t,j} / T)}, \quad
p^{(T)}_s(c) = \frac{\exp(z_{s,c} / T)}{\sum_{j} \exp(z_{s,j} / T)}.
\end{equation}
The KL-divergence from teacher to student is
\begin{equation}
L_{\text{KD}}^{\text{task}}
= \mathrm{KL}\big(p^{(T)}_t \,\|\, p^{(T)}_s\big)
= \sum_{c} p^{(T)}_t(c)
\log \frac{p^{(T)}_t(c)}{p^{(T)}_s(c)}.
\end{equation}
In our setting, we sum over the two tasks and apply the standard $T^2$ scaling:
\begin{equation}
L_{\text{KD}}
= T^2 \Big(
\mathrm{KL}\big(p^{(T)}_{t,\text{mod}} \,\|\, p^{(T)}_{s,\text{mod}}\big)
+
\mathrm{KL}\big(p^{(T)}_{t,\text{loc}} \,\|\, p^{(T)}_{s,\text{loc}}\big)
\Big).
\end{equation}
The temperature $T>1$ smooths the class probabilities, exposing inter-class similarities, while the $T^2$ factor stabilizes gradient magnitudes.\,[web:1]

\subsubsection*{Feature-Level Regression Loss}
Beyond the logits, we also distill intermediate representations from the teacher to the student for both image and text branches.\,[web:1] Let $h^{\text{img}}_{s} \in \mathbb{R}^{d_s}$ and $h^{\text{txt}}_{s} \in \mathbb{R}^{d_s}$ denote the student image and text features (after projection). Let $h^{\text{img,raw}}_{t} \in \mathbb{R}^{d_t}$ and $h^{\text{txt,raw}}_{t} \in \mathbb{R}^{d_t}$ be the corresponding teacher features. Because $d_t$ and $d_s$ may differ, we learn linear projections
\begin{equation}
\tilde{h}^{\text{img}}_{t} = W_{\text{img}} h^{\text{img,raw}}_{t}, \quad
\tilde{h}^{\text{txt}}_{t} = W_{\text{txt}} h^{\text{txt,raw}}_{t},
\end{equation}
with $W_{\text{img}}, W_{\text{txt}} \in \mathbb{R}^{d_s \times d_t}$. The mean squared error loss for the two modalities is
\begin{equation}
L_{\text{MSE}}^{\text{img}}
= \frac{1}{d_s}\big\| h^{\text{img}}_{s} - \tilde{h}^{\text{img}}_{t} \big\|_2^2,
\quad
L_{\text{MSE}}^{\text{txt}}
= \frac{1}{d_s}\big\| h^{\text{txt}}_{s} - \tilde{h}^{\text{txt}}_{t} \big\|_2^2,
\end{equation}
and the combined feature regression term is
\begin{equation} 
L_{\text{MSE}} = L_{\text{MSE}}^{\text{img}} + L_{\text{MSE}}^{\text{txt}}.
\end{equation}

This encourages alignment of the student and teacher embedding spaces for both medical images and clinical text, after dimensionality matching.\,[web:1]

\subsubsection*{Contrastive Representation Distillation Loss}
Finally, we introduce a contrastive representation distillation term that enforces instance-wise alignment between student and teacher representations.\,[web:1] For a batch of size $B$, let
\begin{equation}
\hat{h}^{\text{img}}_{s,i},\ \hat{h}^{\text{txt}}_{s,i} \in \mathbb{R}^{d_s}, \quad
\hat{h}^{\text{img}}_{t,i},\ \hat{h}^{\text{txt}}_{t,i} \in \mathbb{R}^{d_s},
\quad i=1,\dots,B
\end{equation}

denote $\ell_2$-normalized student and teacher features for the $i$-th sample:
\begin{equation}
\hat{h} = \frac{h}{\|h\|_2}.
\end{equation}
For the image branch, we form the similarity matrix
\begin{equation}
\ell^{\text{img}}_{ij}
= \frac{1}{\tau}
\left\langle \hat{h}^{\text{img}}_{s,i},
           \hat{h}^{\text{img}}_{t,j} \right\rangle,
\quad i,j = 1,\dots,B,
\end{equation}
where $\tau>0$ is a contrastive temperature. Using the diagonal as positives (teacher and student embeddings of the same sample) and all off-diagonals as negatives, the image-side contrastive loss is
\begin{equation}
L_{\text{CRD}}^{\text{img}}
= -\frac{1}{B} \sum_{i=1}^{B}
 \log \frac{\exp(\ell^{\text{img}}_{ii})}{
 \sum_{j=1}^{B} \exp(\ell^{\text{img}}_{ij})}.
\end{equation}
An analogous loss is defined for the text branch,
\begin{equation}
\ell^{\text{txt}}_{ij}
= \frac{1}{\tau}
\left\langle \hat{h}^{\text{txt}}_{s,i},
           \hat{h}^{\text{txt}}_{t,j} \right\rangle,
\quad
L_{\text{CRD}}^{\text{txt}}
= -\frac{1}{B} \sum_{i=1}^{B}
 \log \frac{\exp(\ell^{\text{txt}}_{ii})}{
 \sum_{j=1}^{B} \exp(\ell^{\text{txt}}_{ij})}.
\end{equation}
The final CRD term averages both modalities:
\begin{equation}
L_{\text{CRD}}
= \frac{1}{2} \left(
L_{\text{CRD}}^{\text{img}} + L_{\text{CRD}}^{\text{txt}}
\right).
\end{equation}
This instance-discriminative objective encourages each student representation to be closest to its own teacher counterpart in the joint embedding space while being dissimilar to other samples in the batch.\,[web:1]

\subsubsection*{Final Objective}
Combining all components, the proposed distillation objective is
\begin{equation}
L_{\text{total}} =
L_{\text{CE}} +
\alpha\, L_{\text{KD}} +
\beta\, L_{\text{MSE}} +
\gamma\, L_{\text{CRD}},
\end{equation}
which jointly enforces label supervision, soft target alignment at the logit level, feature-space regression, and contrastive instance alignment for both image and text modalities.\,[web:1]


\subsection{Multi-Head Cross-Attention Fusion Architecture}

Both teacher and student models employ identical fusion architectures with multi-head cross-attention between vision and text modalities. The architecture operates on features from separate encoders without shared parameters:

\subsubsection*{Vision Stream:} ViT-Base (teacher) or MobileViT-Small (student) processes image patches independently, producing feature tensor $\mathbf{F}_v \in \mathbb{R}^{N_v \times d}$, where $N_v$ is the number of vision tokens and $d$ is the embedding dimension.

\subsubsection*{Text Stream:} BioClinical-BERT (teacher) or BERT-Mini (student) encodes clinical text independently, producing feature tensor $\mathbf{F}_t \in \mathbb{R}^{N_t \times d}$.

% \textbf{Cross-Attention Fusion:} The multi-head cross-attention computes:

% \begin{equation}
% \text{CrossAttn}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}
% \end{equation}

% We apply this mechanism in both directions (vision-to-text and text-to-vision), concatenating results for fusion. The fusion layer configuration differs between models:
% \begin{itemize}
% \item \textbf{Teacher:} 384-dim fusion, 2 layers, 8 attention heads
% \item \textbf{Student:} 384-dim fusion, 2 layers, 4 attention heads
% \end{itemize}

% \subsection{Cross-Attention Fusion Block}

% To fuse image and text representations, we employ a lightweight cross-attention fusion block that treats the two modalities as a small set of tokens and computes a joint attended representation.\,[web:48] Given an image embedding $h_{\text{img}} \in \mathbb{R}^{d}$ and a text embedding $h_{\text{txt}} \in \mathbb{R}^{d}$ (both already projected to a common dimensionality $d$), we first stack them into a two-token sequence
% \begin{equation}
% X = 
% \begin{bmatrix}
% h_{\text{img}}^{\top} \\
% h_{\text{txt}}^{\top}
% \end{bmatrix}
% \in \mathbb{R}^{2 \times d}.
% \end{equation}

% \subsubsection*{Layer Normalization and Query Construction}
% We form a global summary token by averaging the two modality embeddings and normalizing it:
% \begin{equation}
% \bar{h} = \frac{1}{2} \left(h_{\text{img}} + h_{\text{txt}}\right) \in \mathbb{R}^{d},
% \quad
% \tilde{h} = \mathrm{LN}_1(\bar{h}),
% \end{equation}
% where $\mathrm{LN}_1$ denotes layer normalization.\,[web:48] This summary token serves as the query, while the stacked image--text embeddings $X$ are used as both keys and values:
% \begin{equation}
% Q = \tilde{h} \in \mathbb{R}^{1 \times d},
% \quad
% K = X \in \mathbb{R}^{2 \times d},
% \quad
% V = X \in \mathbb{R}^{2 \times d}.
% \end{equation}

\subsubsection*{Cross-Attention Fusion Block}
The fusion block uses a standard multi-head attention operator with $H$ heads and model dimension $d$.\,[web:48] For each head $h \in \{1, \dots, H\}$, linear projections map inputs into query, key, and value subspaces:
\begin{equation}
Q_h = Q W_h^{Q}, \quad
K_h = K W_h^{K}, \quad
V_h = V W_h^{V},
\end{equation}
where $W_h^{Q}, W_h^{K}, W_h^{V} \in \mathbb{R}^{d \times d_h}$ and $d_h = d / H$. The attention weights for head $h$ are
\begin{equation}
A_h = \mathrm{softmax}\!\left(
    \frac{Q_h K_h^{\top}}{\sqrt{d_h}}
\right) \in \mathbb{R}^{1 \times 2},
\end{equation}
and the corresponding head output is
\begin{equation}
O_h = A_h V_h \in \mathbb{R}^{1 \times d_h}.
\end{equation}
The outputs of all heads are concatenated and projected back to the model dimension:
\begin{equation}
O = \mathrm{Concat}(O_1, \dots, O_H) W^{O} \in \mathbb{R}^{1 \times d},
\end{equation}
with $W^{O} \in \mathbb{R}^{d \times d}$. In the implementation, dropout is optionally applied to the attention output.

\subsubsection*{Feed-Forward Network and Residual Refinement}
The raw attention output is further refined by a position-wise feed-forward network (FFN) with a GELU nonlinearity:
\begin{equation}
\hat{O} = \mathrm{LN}_2(O), 
\end{equation}
\begin{equation}
F(\hat{O}) = \mathrm{Dropout}\!\Big(
  W_2 \, \sigma\big( W_1 \hat{O}^{\top} \big)
\Big)^{\top},
\end{equation}
where $W_1 \in \mathbb{R}^{4d \times d}$, $W_2 \in \mathbb{R}^{d \times 4d}$ are learned weight matrices, $\sigma(\cdot)$ denotes the GELU activation, and $\mathrm{LN}_2$ is a second layer normalization.\,[web:48] A residual connection between the attention output and the FFN output yields the final fused embedding:
\begin{equation}
h_{\text{fused}} = O + F(\hat{O}) \in \mathbb{R}^{1 \times d}.
\end{equation}

Overall, the cross-attention fusion block can be viewed as a single-token multi-head attention query that attends jointly over the image and text tokens, followed by a normalized feed-forward refinement with residual connections:
\begin{equation}
h_{\text{fused}} 
= \mathrm{MHA}\big(\mathrm{LN}_1(\bar{h}), X, X\big)
  + \mathrm{FFN}\!\left(
      \mathrm{LN}_2\big(\mathrm{MHA}(\mathrm{LN}_1(\bar{h}), X, X)\big)
    \right).
\end{equation}
This design produces a compact multimodal representation that integrates information from both the visual and textual branches while keeping the fusion module lightweight enough for edge deployment.\,[web:48]



\subsection{Hyperparameter Optimization Methodology}
We apply automated hyperparameter tuning to select stable and high-performing training settings. The search explores learning rates, loss weights, temperature, and fusion dimensions. Optimization uses a probabilistic sampler with early stopping to discard weak trials.
The best configuration is chosen based on validation average F1 across all tasks. We employ Optuna's Tree-structured Parzen Estimator sampler with the following search space:

\begin{table}
\centering
\caption{Hyperparameter search space and scaling strategy used during optimization.}
\label{tab:hp-tunnning-ss}
\begin{tabular}{lcc}
\toprule
\textbf{Parameter} & \textbf{Range} & \textbf{Scale} \\
\midrule
Teacher Learning Rate & [5e-6, 1e-4] & Log \\
Student Learning Rate & [1e-4, 5e-4] & Log \\
CE Loss Weight ($\alpha$) & [0.5, 2.0] & Linear \\
KD Loss Weight ($\beta$) & [50, 200] & Linear \\
Temperature ($T$) & [2.0, 6.0] & Linear \\
Fusion Dimension & {256, 384} & Categorical \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Optimization Strategy:}
Hyperparameter tuning uses a multivariate TPE sampler to model dependencies between parameters.
A median-based pruner terminates low-performing trials early.
Model selection is based on average F1 score across all tasks for each dataset.
A total of 30 trials are executed, with 40\% pruned through early stopping.

\section{Experimental Setup}

\subsection{Dataset an preprocessing}

\subsubsection*{MedPix-2.0:} 
MedPix 2.0 \cite{abdullah2024medpix} is derived from the open-access MedPix database, curated to include 2,050 high-quality CT (1,062) and MRI (988) images from 671 cases. Images are categorized into five main anatomical locations: Abdomen (319), Head (884), Reproductive and Urinary System (RUS, 158), Thorax (334), and Spine and Muscles (355). The dataset includes structured textual data such as clinical histories, findings, and captions.

We use the provided splits: training (1,653 images, 535 cases) and validation (197 images, 67 cases). Data is loaded from JSONL files for cases and descriptions, with images in PNG format. A custom dataset class caches preprocessed items using CLIP processor for images and DistilBERT tokenizer for texts (max length 128). Labels are mapped numerically for modalities (CT:0, MRI:1) and locations (0-4).
\begin{table}[t]
\centering
\caption{Summary of the MedPix 2.0 dataset used in this study}
\label{tab:medpix_dataset}
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{ll}
\hline
\textbf{Attribute} & \textbf{Details} \\
\hline
Total images & 2,050 \\
Image modalities & CT (1,062), MRI (988) \\
Total cases & 671 \\
\hline
Anatomical locations & 5 \\
 & Abdomen (319) \\
 & Head (884) \\
 & Reproductive and Urinary System (158) \\
 & Thorax (334) \\
 & Spine and Muscles (355) \\
\hline
Textual data & Clinical history, findings, captions \\
\hline
\end{tabular}
\end{table}


\subsubsection*{Wound-1.0:}
We construct a unified wound image dataset by merging several publicly available sources, including Medetec \cite{medetec}, the FUSeg challenge \cite{fuseg}, WSNet-related wound datasets \cite{wsnet}, and the Kaggle Wound Classification dataset \cite{ibrahimfateen2022}. All sources are open access and contain no private or identifiable patient information.

Images are curated to remove duplicates, normalize file formats, and refine annotations. All samples are resized to 
1024×1024 pixels and stored in PNG format. The final dataset contains 1,564 images spanning ten wound categories: abrasions, bruises, burns, cuts, pressure ulcers, diabetic ulcers, lacerations, punctures, surgical wounds, and normal skin.

In addition to categorical labels, we generate synthetic clinical metadata using Qwen2.5-VL-7B-Instruct \cite{qwen2.5-VL}. This includes wound severity levels (mild, moderate, severe) and short textual descriptions, capped at 100 tokens. To ensure clinical validity, all generated attributes are visually reviewed by two registered nurses and one medical officer. A summary of the dataset statistics and class distributions is provided in Table \ref{tab:eda_summary}.

\begin{table*}
\centering
\caption{Summary of The Wound dataset used in this study.}
\label{tab:eda_summary}
\begin{tabular}{ll}
\hline
\textbf{Statistic} & \textbf{Value} \\
\hline
Total images & 1,564 \\
Wound categories & 10 \\
Severity levels & 3 (mild, moderate, severe) \\
\hline
Image resolution & $1024 \times 1024$ pixels \\
\hline
Synthetic description length (mean) & 41.2 words \\
Synthetic description length (min) & 12 words \\
Synthetic description length (max) & 87 words \\
Token limit & 100 \\
\hline
\end{tabular}
\end{table*}


\subsection{Models Evaluated}

We systematically evaluated 11 student architectures combining different vision and text encoders:
\begin{itemize}
\item \textbf{MobileViT variants:} xx-small/small combined with BERT-tiny/mini, MiniLM
\item \textbf{DeiT variants:} tiny/small combined with BERT-tiny/mini, MiniLM  
\item \textbf{DistilBERT variant:} MobileViT-xx-small with DistilBERT
\end{itemize}

All experiments use the same ViT-Base + BioClinical-BERT teacher model.

\subsection{Baseline and Tuned Configurations}

We compared four baseline configurations with four tuned configurations:

\textbf{Baselines (default hyperparameters):}
\begin{itemize}
\item medpix\_fusion\_256: 256-dim fusion dimension
\item medpix\_fusion\_384: 384-dim fusion dimension
\item wound\_fusion\_256: 256-dim fusion dimension
\item wound\_fusion\_384: 384-dim fusion dimension
\end{itemize}

\textbf{Tuned Configurations:}
\begin{itemize}
\item Tuned MedPix: Trial \#11 optimal hyperparameters
\item Tuned Wound: Best hyperparameters for Wound dataset
\end{itemize}

\section{Results and Validation}\label{sec4}

\begin{table}[t]
\centering
\caption{Average F1 score across tasks for tuned student models using knowledge distillation.}
\label{tab:kd_avg_f1}
\begin{tabular}{lc}
\hline
Model & Avg F1 \\
\hline
mobilevit\_xx\_small-bert-mini   & 0.890 \\
mobilevit\_xx\_small-bert-tiny   & 0.870 \\
mobilevit\_small-distilbert      & 0.877 \\
deit\_tiny-minilm                & 0.854 \\
deit\_small-bert-mini            & 0.867 \\
deit\_small-minilm               & 0.832 \\
deit\_tiny-bert-mini             & 0.823 \\
deit\_small-bert-tiny            & 0.842 \\
deit\_small-distilbert           & 0.784 \\
deit\_tiny-distilbert            & 0.715 \\
\hline
\end{tabular}
\end{table}


\begin{table}[t]
\centering
\caption{Effect of fusion dimension on MedPix performance (untuned models).}
\label{tab:fusion_dim_medpix}
\begin{tabular}{lcc}
\hline
Model & Fusion Dim & Avg F1 \\
\hline
mobilevit\_xx\_small-bert-mini & 256 & 0.848 \\
mobilevit\_xx\_small-bert-mini & 384 & 0.872 \\
deit\_tiny-bert-mini           & 256 & 0.831 \\
deit\_tiny-bert-mini           & 384 & 0.856 \\
deit\_small-bert-tiny          & 256 & 0.846 \\
deit\_small-bert-tiny          & 384 & 0.864 \\
\hline
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\caption{Average inference latency for tuned student models across datasets.}
\label{tab:inference_latency}
\begin{tabular}{lcc}
\hline
Model & MedPix (ms) & Wound (ms) \\
\hline
mobilevit\_xx\_small-bert-tiny   & 4.92 & 4.64 \\
mobilevit\_xx\_small-bert-mini   & 5.02 & 4.80 \\
deit\_tiny-bert-mini             & 5.22 & 4.66 \\
deit\_tiny-minilm                & 5.31 & 4.71 \\
deit\_small-bert-mini            & 7.26 & 5.00 \\
deit\_small-distilbert           & 7.30 & 7.11 \\
\hline
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\caption{Performance gap between tuned and untuned models (Avg F1).}
\label{tab:kd_gain}
\begin{tabular}{lccc}
\hline
Model & Untuned Avg F1 & Tuned Avg F1 & Gain \\
\hline
mobilevit\_xx\_small-bert-mini & 0.848 & 0.890 & +0.042 \\
mobilevit\_xx\_small-bert-tiny & 0.845 & 0.870 & +0.025 \\
deit\_tiny-bert-mini           & 0.831 & 0.823 & -0.008 \\
deit\_small-bert-mini          & 0.842 & 0.867 & +0.025 \\
\hline
\end{tabular}
\end{table}




\subsection{Hyperparameter Optimization Results}

% Optuna's optimization process discovered highly consistent patterns among the top 10 trials:

% \begin{table}
% \centering
% \small
% \begin{tabular}{cccccc}
% \toprule
% \textbf{Rank} & \textbf{F1 Score} & \textbf{Teacher LR} & \textbf{Student LR} & \textbf{Alpha} & \textbf{Beta} \\
% \midrule
% 1 & 0.9294 & 4.79e-5 & 1.05e-4 & 0.518 & 112.4 \\
% 2 & 0.9274 & 3.26e-5 & 1.38e-4 & 0.710 & 155.5 \\
% 3 & 0.9242 & 2.41e-5 & 2.00e-4 & 0.937 & 141.8 \\
% 4 & 0.9222 & 6.81e-5 & 1.18e-4 & 0.620 & 116.0 \\
% 5 & 0.9211 & 1.73e-5 & 1.78e-4 & 0.851 & 133.0 \\
% \bottomrule
% \end{tabular}
% \end{table}

% The optimal configuration (Trial \#11) achieved F1 = 0.9294, representing 6-9\% improvement over baseline configurations.

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{hyperparameter_chart.png}
%     \caption{Top Trails by Average F1 score with hyperparameter impact}
%     \label{fig:top-trails-by-f1}
% \end{figure}



% Evaluation across 11 student architectures revealed:
% \ref{fig:model-performance}Model Performance Across Baseline and Tuned Configurations
% \subsection{Model Performance Comparison}
% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{chart.png}
%     \caption{Model Performance Across Baseline and Tuned Configurations}
%     \label{fig:model-performance}
% \end{figure}

\subsection*{MedPix Dataset:}
\begin{itemize}
\item Baseline F1 range: 0.85-0.91
\item Tuned F1 range: 0.86-0.92
\item Best performer: MobileViT-small-bert-mini (92.05\% F1 - location task)
\item Average improvement: 6.2\%
\end{itemize}

\begin{table}[t]
\centering
\caption{Performance comparison for MedPix modality classification}
\label{tab:medpix_modality}
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{lccccc}
\hline
Model Name (Modality) &
Accuracy &
F1 &
Precision &
Recall &
AUC \\
\hline

medpix-mobilevit\_xx\_small-bert-tiny  & 0.9650 & 0.9650 & 0.9650 & 0.9650 & 0.9894 \\
medpix-mobilevit\_xx\_small-bert-mini & 0.9750 & 0.9750 & 0.9750 & 0.9750 & 0.9849 \\
medpix-deit\_tiny-bert-mini           & 0.9400 & 0.9400 & 0.9402 & 0.9400 & 0.9873 \\
medpix-deit\_small-bert-tiny          & 0.9500 & 0.9500 & 0.9507 & 0.9500 & 0.9948 \\
medpix-deit\_tiny-minilm              & 0.9400 & 0.9400 & 0.9407 & 0.9400 & 0.9799 \\
medpix-deit\_small-bert-mini          & 0.9550 & 0.9549 & 0.9572 & 0.9550 & 0.9852 \\
medpix-deit\_small-minilm             & 0.9000 & 0.8999 & 0.9014 & 0.9000 & 0.9654 \\
medpix-deit\_tiny-distilbert          & 0.9350 & 0.9349 & 0.9371 & 0.9350 & 0.9963 \\
medpix-deit\_small-distilbert         & 0.9700 & 0.9700 & 0.9708 & 0.9700 & 0.9971 \\

\hline
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\caption{Performance comparison for MedPix anatomical location classification}
\label{tab:medpix_location}
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{lccccc}
\hline
Model Name (Location) &
Accuracy &
F1 &
Precision &
Recall &
AUC \\
\hline

medpix-mobilevit\_xx\_small-bert-tiny  & 0.7950 & 0.7302 & 0.7678 & 0.7149 & 0.9455 \\
medpix-mobilevit\_xx\_small-bert-mini & 0.8950 & 0.8696 & 0.9139 & 0.8524 & 0.9437 \\
medpix-deit\_tiny-bert-mini           & 0.8750 & 0.8443 & 0.8674 & 0.8321 & 0.9649 \\
medpix-deit\_small-bert-tiny          & 0.8100 & 0.7785 & 0.8415 & 0.7680 & 0.9465 \\
medpix-deit\_tiny-minilm              & 0.8400 & 0.7742 & 0.7854 & 0.7835 & 0.9332 \\
medpix-deit\_small-bert-mini          & 0.7900 & 0.7423 & 0.7808 & 0.7465 & 0.9377 \\
medpix-deit\_small-minilm             & 0.8400 & 0.8089 & 0.8406 & 0.7895 & 0.9586 \\
medpix-mobilevit\_small-distilbert    & 0.7250 & 0.6487 & 0.7535 & 0.6296 & 0.8998 \\
medpix-deit\_tiny-distilbert          & 0.7600 & 0.6757 & 0.6838 & 0.6726 & 0.8981 \\
medpix-deit\_small-distilbert         & 0.8500 & 0.8234 & 0.8146 & 0.8398 & 0.9389 \\

\hline
\end{tabular}
\end{table}








\subsection*{Plots}

\begin{figure*}[t]
\centering

% ---------- Row 1 ----------
\subfloat[\texttt{DeiT-Small + BERT-Tiny}]{
\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_small-bert-tiny/medpix/confusion_location_test.png}
}\hfill
\subfloat[\texttt{DeiT-Small + MiniLM}]{
\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_small-minilm/medpix/confusion_location_test.png}
}\hfill
\subfloat[\texttt{DeiT-Tiny + MiniLM}]{
\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_tiny-minilm/medpix/confusion_location_test.png}
}\hfill
\subfloat[\texttt{DeiT-Tiny + BERT-Mini}]{
\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_tiny-bert-mini/medpix/confusion_location_test.png}
}\hfill
\subfloat[\texttt{MobileViT-XXS + BERT-Mini}]{
\includegraphics[width=0.19\textwidth]{results/plots/Graphs/mobilevit_xx_small-bert-mini/medpix/confusion_location_test.png}
}

\vspace{0.3cm}

% ---------- Row 2 ----------
\subfloat[\texttt{DeiT-Small + DistilBERT}]{
\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_small-distilbert/medpix/confusion_location_test.png}
}\hfill
\subfloat[\texttt{MobileViT-XXS + BERT-Tiny}]{
\includegraphics[width=0.19\textwidth]{results/plots/Graphs/mobilevit_xx_small-bert-tiny/medpix/confusion_location_test.png}
}\hfill
\subfloat[\texttt{MobileViT-Small + DistilBERT}]{
\includegraphics[width=0.19\textwidth]{results/plots/Graphs/mobilevit_small-distilbert/medpix/confusion_location_test.png}
}\hfill
\subfloat[\texttt{DeiT-Tiny + DistilBERT}]{
\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_tiny-distilbert/medpix/confusion_location_test.png}
}\hfill
\subfloat[\texttt{DeiT-Small + BERT-Mini}]{
\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_small-bert-mini/medpix/confusion_location_test.png}
}

\caption{Confusion matrices for MedPix anatomical location classification across student architectures.}
\label{fig:medpix_location_confusion}
\end{figure*}



\begin{figure*}[t]
\centering

% ---------- Row 1 ----------
\subfloat[\texttt{DeiT-Small + BERT-Tiny}]
{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_small-bert-tiny/medpix/confusion_location_test.png}}\hfill
\subfloat[\texttt{DeiT-Small + MiniLM}]
{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_small-minilm/medpix/confusion_location_test.png}}\hfill
\subfloat[\texttt{DeiT-Tiny + MiniLM}]
{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_tiny-minilm/medpix/confusion_location_test.png}}\hfill
\subfloat[\texttt{DeiT-Tiny + BERT-Mini}]
{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_tiny-bert-mini/medpix/confusion_location_test.png}}\hfill
\subfloat[\texttt{MobileViT-XXS + BERT-Mini}]
{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/mobilevit_xx_small-bert-mini/medpix/confusion_location_test.png}}

\vspace{0.3cm}

% ---------- Row 2 ----------
\subfloat[\texttt{DeiT-Small + DistilBERT}]
{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_small-distilbert/medpix/confusion_location_test.png}}\hfill
\subfloat[\texttt{MobileViT-XXS + BERT-Tiny}]
{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/mobilevit_xx_small-bert-tiny/medpix/confusion_location_test.png}}\hfill
\subfloat[\texttt{MobileViT-Small + DistilBERT}]
{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/mobilevit_small-distilbert/medpix/confusion_location_test.png}}\hfill
\subfloat[\texttt{DeiT-Tiny + DistilBERT}]
{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_tiny-distilbert/medpix/confusion_location_test.png}}\hfill
\subfloat[\texttt{DeiT-Small + BERT-Mini}]
{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_small-bert-mini/medpix/confusion_location_test.png}}

\caption{Confusion matrices for anatomical location classification on MedPix across student models.}
\label{fig:medpix_location_confusion}
\end{figure*}

\begin{figure*}[t]
\centering

% ---------- Row 1 ----------
\subfloat[\texttt{DeiT-Small + BERT-Tiny}]
{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_small-bert-tiny/medpix/loss_and_accuracies.png}}\hfill
\subfloat[\texttt{DeiT-Small + MiniLM}]
{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_small-minilm/medpix/loss_and_accuracies.png}}\hfill
\subfloat[\texttt{DeiT-Tiny + MiniLM}]
{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_tiny-minilm/medpix/loss_and_accuracies.png}}\hfill
\subfloat[\texttt{DeiT-Tiny + BERT-Mini}]
{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_tiny-bert-mini/medpix/loss_and_accuracies.png}}\hfill
\subfloat[\texttt{MobileViT-XXS + BERT-Mini}]
{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/mobilevit_xx_small-bert-mini/medpix/loss_and_accuracies.png}}

\vspace{0.3cm}

% ---------- Row 2 ----------
\subfloat[\texttt{DeiT-Small + DistilBERT}]
{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_small-distilbert/medpix/loss_and_accuracies.png}}\hfill
\subfloat[\texttt{MobileViT-XXS + BERT-Tiny}]
{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/mobilevit_xx_small-bert-tiny/medpix/loss_and_accuracies.png}}\hfill
\subfloat[\texttt{MobileViT-Small + DistilBERT}]
{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/mobilevit_small-distilbert/medpix/loss_and_accuracies.png}}\hfill
\subfloat[\texttt{DeiT-Tiny + DistilBERT}]
{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_tiny-distilbert/medpix/loss_and_accuracies.png}}\hfill
\subfloat[\texttt{DeiT-Small + BERT-Mini}]
{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_small-bert-mini/medpix/loss_and_accuracies.png}}

\caption{Training loss and accuracy curves for MedPix across student architectures.}
\label{fig:medpix_training_curves}
\end{figure*}


\subsection*{Wound Dataset:}
\begin{itemize}
\item Baseline F1 range: 0.84-0.88
\item Tuned F1 range: 0.85-0.90
\item Best performer: DeiT-small-minilm (89.66\% F1 - severity task)
\item Average improvement: 5.8\%
\end{itemize}

\subsubsection*{Plots}

\begin{figure*}[t]
\centering

% ---------- Row 1 ----------
\subfloat[DeiT-Small + BERT-Tiny]{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_small-bert-tiny/wound/confusion_type_test.png}}\hfill
\subfloat[DeiT-Small + MiniLM]{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_small-minilm/wound/confusion_type_test.png}}\hfill
\subfloat[DeiT-Tiny + MiniLM]{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_tiny-minilm/wound/confusion_type_test.png}}\hfill
\subfloat[DeiT-Tiny + BERT-Mini]{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_tiny-bert-mini/wound/confusion_type_test.png}}\hfill
\subfloat[MobileViT-XXS + BERT-Mini]{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/mobilevit_xx_small-bert-mini/wound/confusion_type_test.png}}

\vspace{0.3cm}

% ---------- Row 2 ----------
\subfloat[DeiT-Small + DistilBERT]{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_small-distilbert/wound/confusion_type_test.png}}\hfill
\subfloat[MobileViT-XXS + BERT-Tiny]{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/mobilevit_xx_small-bert-tiny/wound/confusion_type_test.png}}\hfill
\subfloat[MobileViT-Small + DistilBERT]{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/mobilevit_small-distilbert/wound/confusion_type_test.png}}\hfill
\subfloat[DeiT-Tiny + DistilBERT]{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_tiny-distilbert/wound/confusion_type_test.png}}\hfill
\subfloat[DeiT-Small + BERT-Mini]{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_small-bert-mini/wound/confusion_type_test.png}}

\caption{Confusion matrices for wound type classification across student architectures.}
\label{fig:wound_type_confusion}
\end{figure*}


\begin{figure*}[t]
\centering

% ---------- Row 1 ----------
\subfloat[DeiT-Small + BERT-Tiny]{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_small-bert-tiny/wound/confusion_severity_test.png}}\hfill
\subfloat[DeiT-Small + MiniLM]{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_small-minilm/wound/confusion_severity_test.png}}\hfill
\subfloat[DeiT-Tiny + MiniLM]{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_tiny-minilm/wound/confusion_severity_test.png}}\hfill
\subfloat[DeiT-Tiny + BERT-Mini]{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_tiny-bert-mini/wound/confusion_severity_test.png}}\hfill
\subfloat[MobileViT-XXS + BERT-Mini]{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/mobilevit_xx_small-bert-mini/wound/confusion_severity_test.png}}

\vspace{0.3cm}

% ---------- Row 2 ----------
\subfloat[DeiT-Small + DistilBERT]{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_small-distilbert/wound/confusion_severity_test.png}}\hfill
\subfloat[MobileViT-XXS + BERT-Tiny]{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/mobilevit_xx_small-bert-tiny/wound/confusion_severity_test.png}}\hfill
\subfloat[MobileViT-Small + DistilBERT]{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/mobilevit_small-distilbert/wound/confusion_severity_test.png}}\hfill
\subfloat[DeiT-Tiny + DistilBERT]{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_tiny-distilbert/wound/confusion_severity_test.png}}\hfill
\subfloat[DeiT-Small + BERT-Mini]{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_small-bert-mini/wound/confusion_severity_test.png}}

\caption{Confusion matrices for wound severity classification across student architectures.}
\label{fig:wound_severity_confusion}
\end{figure*}


\begin{figure*}[t]
\centering

% ---------- Row 1 ----------
\subfloat[DeiT-Small + BERT-Tiny]{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_small-bert-tiny/wound/loss_and_accuracies.png}}\hfill
\subfloat[DeiT-Small + MiniLM]{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_small-minilm/wound/loss_and_accuracies.png}}\hfill
\subfloat[DeiT-Tiny + MiniLM]{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_tiny-minilm/wound/loss_and_accuracies.png}}\hfill
\subfloat[DeiT-Tiny + BERT-Mini]{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_tiny-bert-mini/wound/loss_and_accuracies.png}}\hfill
\subfloat[MobileViT-XXS + BERT-Mini]{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/mobilevit_xx_small-bert-mini/wound/loss_and_accuracies.png}}

\vspace{0.3cm}

% ---------- Row 2 ----------
\subfloat[DeiT-Small + DistilBERT]{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_small-distilbert/wound/loss_and_accuracies.png}}\hfill
\subfloat[MobileViT-XXS + BERT-Tiny]{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/mobilevit_xx_small-bert-tiny/wound/loss_and_accuracies.png}}\hfill
\subfloat[MobileViT-Small + DistilBERT]{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/mobilevit_small-distilbert/wound/loss_and_accuracies.png}}\hfill
\subfloat[DeiT-Tiny + DistilBERT]{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_tiny-distilbert/wound/loss_and_accuracies.png}}\hfill
\subfloat[DeiT-Small + BERT-Mini]{\includegraphics[width=0.19\textwidth]{results/plots/Graphs/deit_small-bert-mini/wound/loss_and_accuracies.png}}

\caption{Training loss and accuracy curves for wound classification across student models.}
\label{fig:wound_training_curves}
\end{figure*}




\begin{table}
\centering
\caption{Performance comparison for wound type classification}
\label{tab:wound_type_summary}
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{lccccc}
\hline
Model Name (Type) &
Accuracy &
F1 &
Precision &
Recall &
AUC \\
\hline

wound-mobilevit\_xx\_small-bert-tiny  & 0.8638 & 0.8270 & 0.8844 & 0.8270 & 0.9838 \\
wound-mobilevit\_xx\_small-bert-mini & 0.8383 & 0.8575 & 0.9114 & 0.8346 & 0.9860 \\
wound-deit\_tiny-bert-mini           & 0.7787 & 0.7264 & 0.7168 & 0.7578 & 0.9745 \\
wound-deit\_small-bert-tiny          & 0.8128 & 0.7941 & 0.8324 & 0.7955 & 0.9843 \\
wound-deit\_tiny-minilm              & 0.7745 & 0.7813 & 0.7995 & 0.7699 & 0.9792 \\
wound-deit\_small-bert-mini          & 0.8043 & 0.8205 & 0.8357 & 0.8107 & 0.9644 \\
wound-deit\_small-minilm             & 0.7957 & 0.7319 & 0.8066 & 0.7089 & 0.9675 \\
wound-deit\_tiny-distilbert          & 0.7149 & 0.6798 & 0.6986 & 0.6956 & 0.9622 \\
wound-deit\_small-distilbert         & 0.8298 & 0.8201 & 0.8114 & 0.8434 & 0.9822 \\

\hline
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\caption{Performance comparison for wound severity classification}
\label{tab:wound_severity}
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{lccccc}
\hline
Model Name (Severity) &
Accuracy &
F1 &
Precision &
Recall &
AUC \\
\hline

wound-mobilevit\_xx\_small-bert-tiny  & 0.9234 & 0.9129 & 0.9424 & 0.8911 & 0.9811 \\
wound-mobilevit\_xx\_small-bert-mini & 0.9319 & 0.9223 & 0.9205 & 0.9254 & 0.9890 \\
wound-deit\_tiny-bert-mini           & 0.9191 & 0.9194 & 0.9039 & 0.9385 & 0.9826 \\
wound-deit\_small-bert-tiny          & 0.9106 & 0.8893 & 0.9334 & 0.8609 & 0.9837 \\
wound-deit\_tiny-minilm              & 0.9234 & 0.9264 & 0.9308 & 0.9226 & 0.9765 \\
wound-deit\_small-bert-mini          & 0.9277 & 0.9129 & 0.9177 & 0.9106 & 0.9893 \\
wound-deit\_small-minilm             & 0.9319 & 0.9330 & 0.9289 & 0.9373 & 0.9918 \\
wound-mobilevit\_small-distilbert    & 0.8851 & 0.8650 & 0.8702 & 0.8740 & 0.9744 \\
wound-deit\_tiny-distilbert          & 0.8043 & 0.7504 & 0.7394 & 0.8027 & 0.9448 \\
wound-deit\_small-distilbert         & 0.7745 & 0.7475 & 0.7660 & 0.7642 & 0.9442 \\

\hline
\end{tabular}
\end{table}



\subsubsection*{Loss Explore}
Table \ref{tab:loss_ablation_medpix}
\begin{table*}[t]
\centering
\caption{Effect of different knowledge distillation loss functions on Medpix dataset test performance.}
\label{tab:loss_ablation_medpix}
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{lcccccc}
\hline
Loss &
Modality Acc &
Location Acc &
Modality F1 &
Location F1 &
Final Loss \\
\hline

combined 

& 0.9800 & 0.8500 & 0.9800 & 0.7945
& 14.81 \\

crd 
& 0.4700 & 0.1200 & 0.4700 & 0.1144
& 0.41 \\

mmd 
& 0.6150 & 0.1900 & 0.5998 & 0.1202
& 0.12 \\

rkd 
& 0.6200 & 0.2000 & 0.6087 & 0.0879
& 0.59 \\

vanilla 
& 0.9350 & 0.7950 & 0.9350 & 0.7591
& 3.09 \\

\hline
\end{tabular}
\end{table*}

\begin{table}[t]
\centering
\caption{Effect of different knowledge distillation loss functions on wound dataset test performance.}
\label{tab:loss_ablation_wound}
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{lcccccc}
\hline
Loss &
Type Acc &
Severity Acc &
Type F1 &
Severity F1 &
% Type AUC &
% Severity AUC &
% Infer (ms) &
Final Loss \\
\hline

combined
& 0.8851 & 0.9319 & 0.8785 & 0.9147 & 17.57 \\

crd
& 0.1064 & 0.5575 & 0.0955 & 0.4178 & 0.67 \\

mmd
& 0.0851 & 0.3660 & 0.0395 & 0.2863 & 0.13 \\

rkd
& 0.1532 & 0.6043 & 0.0664 & 0.4826 & 0.74 \\

vanilla
& 0.8128 & 0.9489 & 0.8105 & 0.9560 & 4.53 \\

\hline
\end{tabular}
\end{table}





% Fusion strategy comparison on the MedPix test split using a MobileViT-xx-small + BERT-mini student model with combined loss.

% \begin{table*}[t]
% \centering
% \caption{Fusion strategy comparison on the MedPix test split using a MobileViT-xx-small + BERT-mini student model.}
% \label{tab:fusion_explore_test}
% \setlength{\tabcolsep}{4pt}
% \renewcommand{\arraystretch}{1.1}
% \begin{tabular}{lcccccccc}
% \hline
% Fusion &
% Mod Acc &
% Loc Acc &
% Mod F1 &
% Loc F1 &
% Mod AUC &
% Loc AUC &
% Infer (ms) & \\
% \hline

% cross\_attention
% & 0.960 & 0.880 & 0.9600 & 0.8427 & 0.9954 & 0.9542 & 4.83  \\

% concat\_mlp
% & 0.955 & 0.875 & 0.9550 & 0.8475 & 0.9880 & 0.9595 & 4.83  \\

% energy\_aware\_adaptive
% & 0.905 & 0.765 & 0.9049 & 0.6939 & 0.9612 & 0.9212 & 4.98 \\

% film
% & 0.975 & 0.860 & 0.9750 & 0.8132 & 0.9932 & 0.9504 & 4.97 \\

% gated
% & 0.980 & 0.860 & 0.9800 & 0.8313 & 0.9919 & 0.9657 & 4.80 \\

% modality\_dropout
% & 0.955 & 0.855 & 0.9549 & 0.8248 & 0.9938 & 0.9654 & 5.00 \\

% shomr
% & 0.970 & 0.725 & 0.9700 & 0.5839 & 0.9942 & 0.8739 & 4.94 \\

% simple
% & 0.985 & 0.845 & 0.9850 & 0.8128 & 0.9905 & 0.9489 & 4.82 \\

% transformer\_concat
% & 0.970 & 0.865 & 0.9700 & 0.8417 & 0.9914 & 0.9468 & 4.93 \\

% \hline
% \end{tabular}
% \end{table*}



\subsection{Efficiency Analysis and Proposed Student Model}

% [Figure 2: Parameter Count vs Performance Trade-off Analysis]
% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{chart_1.png}
%     \caption{Parameter Count vs Performance Trade-off Analysis}
%     \label{fig:parameter-count}
% \end{figure}

\paragraph{Parameter efficiency.\cite{zhang2020accuracyefficiency}}
To compare models not only by raw predictive performance but also by how effectively they use their capacity, we report a simple parameter-efficiency metric, defined as F1-score per million parameters of the student network.[file:1][web:69]
Let $\mathrm{F1}_{\text{avg}}$ denote the average macro F1-score over tasks (e.g., modality and location), and let $\mathrm{Params}_{\text{M}}$ be the number of trainable student parameters measured in millions.
We define
\begin{equation}
\mathrm{ParamEff}
= \frac{\mathrm{F1}_{\text{avg}}}{\mathrm{Params}_{\text{M}}}.
\end{equation}
Higher values indicate models that achieve better predictive performance per parameter, i.e., a more favorable accuracy–efficiency trade-off, similar in spirit to prior work that studies accuracy versus computational or parameter cost in neural architectures.[web:69]


The efficiency analysis identified MobileViT-small-bert-mini as the optimal ultra-edge deployment candidate:

\begin{table}[H]
\centering
\caption{Performance Comparison of Top Student Architectures. MobileViT-small-bert-mini achieves superior parameter efficiency (53.1 F1/M) with 75\% fewer parameters than DeiT-small while maintaining the highest F1 score (0.9108) and fastest inference (4.76ms), making it optimal for edge deployment.}
\label{tab:compression-stats}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Metric} & \textbf{MobileViT-small} & \textbf{DeiT-small} & \textbf{DistilBERT} \\
\midrule
Parameters (M) & 17.13 & 45.52 & 68.38 \\
Avg F1 Score & 0.9108 & 0.8966 & 0.8658 \\
Inference (ms) & 4.76 & 5.49 & 5.54 \\
Param Efficiency & 53.1 F1/M & 19.7 F1/M & 12.7 F1/M \\
\bottomrule
\end{tabular}
\end{table}

MobileViT-small-bert-mini as we can observe from \ref{tab:compression} and \ref{tab:compression-stats} achieves the optimal balance with 11.1× parameter compression (17.7M vs. teacher's 196.5M) while retaining 97.5\% of teacher performance (F1: 0.905 vs. 0.927), outperforming smaller models that sacrifice 6-7\% accuracy for higher compression ratios. The tuned variant further improves to 97.7\% retention (F1: 0.908) with superior parameter efficiency (0.050 F1/M), making it ideal for real-time edge deployment in clinical settings. No other architecture matches this accuracy-efficiency trade-off across both MedPix and Wound datasets.
\begin{table}[t]
\centering
\caption{Model compression analysis comparing teacher and student models.}
\label{tab:compression_analysis}
\begin{tabular}{lccc}
\hline
Model &
Teacher Params (M) &
Student Params (M) &
Compression Ratio \\
\hline
mobilevit\_xx\_small-bert-tiny  & 195.9 & 6.25 & 31.3$\times$ \\
mobilevit\_xx\_small-bert-mini & 195.9 & 13.06 & 15.0$\times$ \\
deit\_tiny-bert-mini           & 195.9 & 17.64 & 11.1$\times$ \\
deit\_small-bert-mini          & 195.9 & 33.94 & 5.8$\times$ \\
deit\_small-distilbert         & 195.9 & 89.26 & 2.2$\times$ \\
\hline
\end{tabular}
\end{table}


% \begin{table}[t]
% \centering
% \caption{Teacher-to-Student Compression. MobileViT-small-bert-mini (bold) achieves optimal 11.1$\times$ compression with 97.7\% performance retention.}
% \label{tab:compression}
% \scriptsize
% \begin{tabular}{lrrrrrrr}
% \toprule
% Model & T.P. & S.P. & Comp. & T.F1 & S.F1 & Ret. & Eff. \\
%  & (M) & (M) & ($\times$) & & & (\%) & \\
% \midrule
% mobilevit-xx-small-bert-tiny & 196.5 & 6.8 & \textbf{28.9} & 0.927 & 0.870 & 93.8 & 0.128 \\
% deit-tiny-bert-tiny & 196.5 & 11.3 & 17.4 & 0.927 & 0.860 & 92.8 & 0.076 \\
% mobilevit-xx-small-bert-mini & 196.5 & 13.7 & 14.3 & 0.927 & 0.873 & 94.2 & 0.064 \\
% \textbf{mobilevit-small-bert-mini} & \textbf{196.5} & \textbf{17.7} & \textbf{11.1} & \textbf{0.927} & \textbf{0.905} & \textbf{97.5} & \textbf{0.051} \\
% deit-tiny-bert-mini & 196.5 & 18.2 & 10.8 & 0.927 & 0.870 & 93.8 & 0.048 \\
% mobilevit-xx-small-minilm & 196.5 & 25.2 & 7.8 & 0.927 & 0.874 & 94.3 & 0.035 \\
% deit-small-bert-tiny & 196.5 & 27.7 & 7.1 & 0.927 & 0.866 & 93.4 & 0.031 \\
% mobilevit-small-minilm & 196.5 & 29.3 & 6.7 & 0.927 & 0.898 & 96.9 & 0.031 \\
% deit-tiny-minilm & 196.5 & 29.8 & 6.6 & 0.927 & 0.862 & 93.0 & 0.029 \\
% deit-small-minilm & 196.5 & 46.1 & 4.3 & 0.927 & 0.891 & 96.1 & 0.019 \\
% mobilevit-xx-small-distilbert & 196.5 & 68.9 & 2.9 & 0.927 & 0.888 & 95.8 & 0.013 \\
% \midrule
% \textbf{mobilevit-small-bert-mini (tuned)} & \textbf{197.1} & \textbf{18.2} & \textbf{10.8} & \textbf{0.929} & \textbf{0.908} & \textbf{97.7} & \textbf{0.050} \\
% \bottomrule
% \end{tabular}
% \vspace{-2mm}
% \end{table}


\subsection{Ablation Studies}
We analyze the effect of knowledge distillation, fusion dimensionality, and architectural choice across two medical datasets. Results show that student models trained exclusively on teacher logits achieve strong generalization without feature sharing. MobileViT-based students consistently outperform DeiT variants under identical distillation settings, achieving up to a 4.2-point F1 gain after tuning. While increasing fusion dimensionality improves untuned performance, tuned models recover accuracy with a compact 256-dimensional fusion, preserving efficiency. Inference analysis confirms sub-5 ms latency for the best performing students, validating suitability for edge deployment.


\textbf{Fusion Dimension Analysis:}
\begin{itemize}
\item 256-dim: 87.3\% avg F1 (baseline)
\item 384-dim: 89.1\% avg F1 (optimal across trials)
\item 512-dim: 86.8\% avg F1 (over-parameterization evident)
\end{itemize}

\textbf{Attention Heads Configuration:}
\begin{itemize}
\item Teacher with 8 heads, Student with 4 heads: Consistently superior
\item 2:1 capacity ratio enables effective knowledge transfer
\end{itemize}

\textbf{Dropout Regularization:}
\begin{itemize}
\item Baseline dropout: 0.1 (teacher and student)
\item Tuned dropout: 0.185 (teacher), 0.238 (student)
\item Impact: Significant generalization improvement, particularly on Wound dataset
\end{itemize}

\section{Discussion}

\subsection{Key Findings}

This work demonstrates that systematic hyperparameter optimization combined with multimodal knowledge distillation produces substantial performance improvements for medical image classification on edge devices. Several key insights emerged:

\textbf{1. Optimal Learning Rate Asymmetry:} The best configuration uses teacher learning rate (4.79e-5) substantially higher than typical, approximately 5x the default. This enables the large teacher to converge effectively within 3 epochs. Conversely, the student learning rate (1.05e-4) is 3.3x lower than standard, promoting stability during knowledge transfer. This asymmetry reflects the different optimization landscapes of knowledge distillation versus standard supervised learning.

\textbf{2. Fusion Dimension Sweet Spot:} Both teacher and student models consistently preferred 384-dimensional fusion spaces, rejecting the baseline 512-dimension as over-parameterized. The 384-dim configuration balances representational capacity with computational efficiency, particularly important for edge deployment.

\textbf{3. Temperature-Regulated Knowledge Transfer:} Optimal temperature values clustered in the 2.5-3.5 range, substantially lower than the theoretical maximum (6.0). This indicates that moderate softening of teacher probability distributions (temperature 3.19) effectively transfers knowledge without over-regularization. Temperatures outside this range led to performance degradation.

\textbf{4. MobileViT Superiority in Medical Imaging:} Across all efficiency metrics, MobileViT-based architectures consistently outperformed DeiT variants with equivalent parameter counts. The hybrid CNN-Transformer design of MobileViT appears particularly suited to medical images, possibly due to better capture of local anatomical details through its convolutional components.

\textbf{5. Regularization Through Dropout:} Contrary to common practice of minimizing dropout in student models, optimal configurations employed significantly higher dropout rates (0.238 for student). This aggressive regularization appears to prevent overfitting to teacher outputs, improving generalization on held-out data.

\subsection{Clinical Applicability}

The proposed MobileViT-small-bert-mini model achieves practical deployment viability:
\begin{itemize}
\item \textbf{Mobile Deployment:} 17.13M parameters enables execution on modern smartphones (typical RAM: 6-12GB)
\item \textbf{Edge Inference:} 4.76ms inference latency on standard edge hardware (Qualcomm Snapdragon, ARM processors) enables real-time diagnostic support
\item \textbf{Multimodal Input:} Native support for image + clinical notes, improving diagnostic accuracy through contextual understanding
\end{itemize}

\subsection{Limitations}

\textbf{1. Development Set Evaluation:} Our study uses development sets for both optimization and validation. While Optuna's pruning prevents overfitting, evaluation on truly held-out test sets would strengthen conclusions regarding generalization.

\textbf{2. Dataset Diversity:} Both datasets, while representing different medical imaging domains, share characteristics of annotated medical data. Evaluation on additional datasets (histopathology, radiology reports, etc.) would better establish generalization.

\textbf{3. Multimodal Text Analysis:} Our multimodal approach relies on structured clinical annotations. Performance with free-text medical notes or radiology reports remains unexplored.

\textbf{4. Teacher Architecture Fixed:} While student architectures were systematically varied, the teacher model remained fixed. Exploring different teacher architectures could potentially improve student performance.

\section{Conclusion and Future Work}\label{sec5}

This work presents a comprehensive framework for deploying efficient, multimodal medical image classification systems through knowledge distillation and systematic hyperparameter optimization. Key contributions include: (1) a two-phase knowledge distillation methodology with explicit architectural separation of teacher and student; (2) multi-head cross-attention fusion mechanisms enabling effective multimodal learning; (3) application of Optuna-based hyperparameter optimization discovering a 92.94\% validation F1 configuration; and (4) identification of MobileViT-small-bert-mini as an optimal ultra-edge deployment solution.

The 6-9\% performance improvement over baselines, combined with sub-5ms inference latency and 17.13M parameters, makes the proposed approach suitable for real-world deployment in resource-constrained clinical environments. Future work should explore: (1) evaluation on truly held-out test sets and additional medical imaging datasets; (2) investigation of interpretability mechanisms for clinical validation; (3) extension to video medical data (ultrasound, endoscopy); and (4) federated learning approaches for privacy-preserving multimodal medical imaging analysis.










%% Loading bibliography style file
%\bibliographystyle{model1-num-names}
%\bibliographystyle{cas-model2-names}
\bibliographystyle{apalike}
% Loading bibliography database
\bibliography{bibilography}

% Biography
%\bio{}
% Here goes the biography details.
%\endbio

%\bio{pic1}
% Here goes the biography details.
%\endbio

\end{document}

