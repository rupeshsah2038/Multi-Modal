@article{pham2023mskd,
  author  = {Pham, T. H. and others},
  title   = {MSKD: Structured knowledge distillation for efficient medical image segmentation},
  journal = {Medical Image Analysis},
  year    = {2023}
}

@article{chen2025vlmreview,
  author  = {Chen, Z. and others},
  title   = {Vision-language foundation models for medical imaging: a review},
  journal = {IEEE Transactions on Medical Imaging},
  year    = {2025}
}

@thesis{bertresnetcrossattention,
  title  = {BERT-ResNet Cross-Attention Fusion Network for Multimodal Sentiment Classification},
  school = {Rochester Institute of Technology},
  year   = {unknown}
}

@article{gou2024kdreview,
  author  = {Gou, J. and others},
  title   = {Knowledge distillation and teacher-student learning in medical imaging},
  journal = {Nature Reviews Biomedical Engineering},
  year    = {2024}
}

@article{kervadec2022jmi,
  author  = {Kervadec, H. and others},
  title   = {Knowledge distillation with ensembles for medical image segmentation},
  journal = {Journal of Medical Imaging},
  volume  = {9},
  number  = {5},
  year    = {2022}
}

@article{kervadec2022pmc,
  author  = {Kervadec, H. and others},
  title   = {Knowledge distillation with ensembles of convolutional neural networks for medical image segmentation},
  journal = {PMC},
  year    = {2022}
}

@article{kumar2025vitreview,
  author  = {Kumar, R. and others},
  title   = {Vision Transformers in Medical Imaging: A Comprehensive Review},
  journal = {PMC},
  year    = {2025}
}

@online{ishikawa2020tpe,
  author  = {Ishikawa, K.},
  title   = {Multivariate TPE makes Optuna even more powerful},
  year    = {2020},
  note    = {Preferred Networks Blog}
}

@article{ravichandran2024vit2d,
  author  = {Ravichandran, K. and others},
  title   = {Implementing vision transformer for classifying 2D biomedical images},
  journal = {Scientific Reports},
  year    = {2024}
}

@inproceedings{mehta2022mobilevit,
  author    = {Mehta, S. and Rastegari, M.},
  title     = {MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer},
  booktitle = {International Conference on Learning Representations},
  year      = {2022}
}

@online{bioclinicalmpbert,
  title = {BioClinicalMPBERT Model},
  note  = {Dataloop AI Library},
  year  = {unknown}
}

@article{touvron2020deit,
  author  = {Touvron, H. and others},
  title   = {Training data-efficient image transformers and distillation through attention},
  journal = {Meta AI Research},
  year    = {2020}
}

@inproceedings{multitaskiccs2025,
  title     = {Enhancing Medical Image Analysis with Multi-Task Learning},
  booktitle = {ICCS},
  year      = {2025}
}

@inproceedings{ling2023bioclinicalbert,
  author    = {Ling, Y. and others},
  title     = {Bio+Clinical BERT performance on medical sentiment analysis},
  booktitle = {KDD DSHealth},
  year      = {2023}
}

@techreport{duvvuru2023deit,
  author = {Duvvuru, N.},
  title  = {Data Efficient Image Transformer (DeiT) Technical Review},
  year   = {2023}
}

@article{lee2019biobert,
  author  = {Lee, J. and others},
  title   = {BioBERT: A pre-trained biomedical language representation model},
  journal = {Bioinformatics},
  year    = {2019}
}

@online{codingdeit,
  title = {Coding DeiT Data Efficient Image Transformer},
  note  = {Vizuara AI Platform},
  year  = {unknown}
}

@article{xuanyu2020multitaskct,
  author  = {Xuanyu, T. and others},
  title   = {Multi-task deep learning based CT imaging analysis},
  journal = {PMC},
  year    = {2020}
}

@inproceedings{zhang2020accuracyefficiency,
  author    = {Zhang, X. and others},
  title     = {Towards Better Accuracy-Efficiency Trade-offs: Divide and Co-training},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2020}
}
@article{abdullah2024medpix,
  title={Medpix 2.0: a comprehensive multimodal biomedical dataset for advanced AI applications},
  author={Siragusa, Irene and Contino, Salvatore and La Ciura, Massimo and Alicata, Rosario and Pirrone, Roberto},
  journal={arXiv preprint arXiv:2407.02994},
  pages={16},
  year={2024}
}

References in BibTeX Format
@article{Nussbaum2018,
author = {Nussbaum, S. R. and Carter, M. J. and Fife, C. E. and DaVanzo, J. and Haught, R. and Nusgart, M. and Cartwright, D.},
title = {An Economic Evaluation of the Impact, Cost, and Medicare Policy Implications of Chronic Nonhealing Wounds},
journal = {Value in Health},
year = {2018},
volume = {21},
number = {1},
pages = {27--32},
doi = {10.1016/j.jval.2017.07.007}
}
@article{Jarbrink2017,
author = {Järbrink, Krister and Ni, Gao and Sönnergren, Henrik and Schmidtchen, Artur and Pang, Caroline and Bajpai, Ram and Car, Josip},
title = {The humanistic and economic burden of chronic wounds: a protocol for a systematic review},
journal = {Systematic Reviews},
year = {2017},
volume = {6},
number = {1},
pages = {15},
doi = {10.1186/s13643-016-0400-8}
}
@article{Hassan2022,
author = {Hassan, Ahmed and Makhlouf, Youssef and Schmid, Oliver and Roth, Volker},
title = {Detect-and-segment: A deep learning approach to automate wound image segmentation},
journal = {Informatics in Medicine Unlocked},
year = {2022},
volume = {29},
pages = {100884},
doi = {10.1016/j.imu.2022.100884}
}
@article{Wang2020,
author = {Wang, Chao and Anisuzzaman, S. and Williamson, Victor and Pang, Mrinal and Gryak, Jonathan and Najarian, Kayvan},
title = {Fully automatic wound segmentation with deep convolutional neural networks},
journal = {Scientific Reports},
year = {2020},
volume = {10},
number = {1},
pages = {21897},
doi = {10.1038/s41598-020-78799-w}
}
@article{Zhao2023,
author = {Zhao, Yan and Li, Qichang and Li, Yongjie and Zhang, Tingting and Han, Jing and Shen, Zhiyong and Li, Jianpeng and Shan, Pengfei},
title = {Multi-task deep learning for medical image computing and analysis: A review},
journal = {Computers in Biology and Medicine},
year = {2023},
volume = {153},
pages = {106496},
doi = {10.1016/j.compbiomed.2022.106496}
}
@misc{Nasim2024,
author = {Nasim, MD Abdullah Al and Rayhana, Rakibunnessa and Alauddin, Md and Sarker, Niloy and Ahamad, Md Martuza and Svensson, Torgil R. Mollick},
title = {Deep Learning for Automated Wound Classification And Segmentation},
year = {2024},
eprint = {2408.11064},
archivePrefix = {arXiv},
primaryClass = {eess.IV}
}
@article{Huang2022,
author = {Huang, Aiyue and Jiang, Li and Zhang, Jiangshan and Wang, Qing},
title = {Attention-VGG16-UNet: a novel deep learning approach for automatic segmentation of the median nerve in ultrasound images},
journal = {Quantitative Imaging in Medicine and Surgery},
year = {2022},
volume = {12},
number = {6},
pages = {3138--3150},
doi = {10.21037/qims-21-1074}
}
@article{Chen2018,
author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
title = {Rethinking Atrous Convolution for Semantic Image Segmentation},
journal = {arXiv preprint arXiv:1706.05587},
year = {2017}
}
@article{Wagh2020,
author = {Wagh, Ameya and Jain, Shubham and Mukherjee, Apratim and Agu, Emmanuel and Pedersen, Peder and Strong, Diane and Tulu, Bengisu and Lindsay, Clifford and Liu, Ziyang},
title = {Semantic Segmentation of Smartphone Wound Images: Comparative Analysis of AHRF and CNN-Based Approaches},
journal = {IEEE Access},
year = {2020},
volume = {8},
pages = {181590--181604},
doi = {10.1109/ACCESS.2020.3024175}
}
@article{Bernardino2024,
author = {Bernardino, Catarina and Mendes, Rui and Fonseca, Miguel and Pereira, Tania and Oliveira, Hugo Pedro},
title = {Exploration of multi-task methodologies for chronic wound image analysis},
journal = {Dissertation, University of Porto},
year = {2024},
url = {https://repositorio-aberto.up.pt/bitstream/10216/163290/2/697467.pdf}
}
@article{Chen2017,
author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
title = {DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
year = {2017},
volume = {40},
number = {4},
pages = {834--848},
doi = {10.1109/TPAMI.2017.2699184}
}
@article{Anisuzzaman2022,
author = {Anisuzzaman, D. M. and Wang, Chuanbo and Rostami, Babak and Gopalakrishnan, Sandeep and Niezgoda, Jeffrey and Yu, Zeyun},
title = {Multi-modal wound classification using wound image and location by deep neural network},
journal = {Scientific Reports},
year = {2022},
volume = {12},
number = {1},
pages = {20057},
doi = {10.1038/s41598-022-21813-0}
}
@article{Choi2023,
author = {Choi, Joonmyoung and Kim, Hyunjin and Min, Sunjae and Kim, Changick},
title = {Development of a deep learning-based tool to assist wound classification},
journal = {Burns},
year = {2023},
volume = {49},
number = {3},
pages = {720--726},
doi = {10.1016/j.burns.2022.05.018}
}
@article{Carmona2023,
author = {Carmona, Miguel and García-Betances, Rebeca Isabel and Ortiz, Miguel A. Vaquero-Blasco and Arredondo, María Teresa and Mugica, Fernando and García, Pablo and others},
title = {A Multitask Deep Learning Approach for Staples and Wound Segmentation in Abdominal Surgery Images},
journal = {International Conference of the Italian Association for Artificial Intelligence},
year = {2023},
pages = {255--268},
doi = {10.1007/978-3-031-39965-7_18}
}
@article{Hu2025,
author = {Hu, Y. and Zhang, Y. and Li, Z. and Wang, H. and Li, J. and Zhang, L. and others},
title = {Wound Segmentation with U-Net Using a Dual Attention Mechanism and VGG16},
journal = {Journal of Digital Imaging},
year = {2025},
doi = {10.1007/s10278-025-01386-w}
}
@article{Anisuzzaman2025,
author = {Anisuzzaman, D. M. and Patel, Yash and Rostami, Babak and Niezgoda, Jeffrey and Gopalakrishnan, Sandeep and Yu, Zeyun},
title = {Multi-modal wound classification using wound image and location by deep neural network},
journal = {arXiv preprint arXiv:2505.08086},
year = {2025}
}
@article{Nussbaum2018,
author = {Nussbaum, S. R. and Carter, M. J. and Fife, C. E. and DaVanzo, J. and Haught, R. and Nusgart, M. and Cartwright, D.},
title = {An Economic Evaluation of the Impact, Cost, and Medicare Policy Implications of Chronic Nonhealing Wounds},
journal = {Value in Health},
year = {2018},
volume = {21},
number = {1},
pages = {27--32},
doi = {10.1016/j.jval.2017.07.007}
}
@article{Wang2020,
author = {Wang, Chao and Anisuzzaman, S. and Williamson, Victor and Pang, Mrinal and Gryak, Jonathan and Najarian, Kayvan},
title = {Fully automatic wound segmentation with deep convolutional neural networks},
journal = {Scientific Reports},
year = {2020},
volume = {10},
number = {1},
pages = {21897},
doi = {10.1038/s41598-020-78799-w}
}
@article{Hassan2022,
author = {Hassan, Ahmed and Makhlouf, Youssef and Schmid, Oliver and Roth, Volker},
title = {Detect-and-segment: A deep learning approach to automate wound image segmentation},
journal = {Informatics in Medicine Unlocked},
year = {2022},
volume = {29},
pages = {100884},
doi = {10.1016/j.imu.2022.100884}
}
@article{Huang2022,
author = {Huang, Aiyue and Jiang, Li and Zhang, Jiangshan and Wang, Qing},
title = {Attention-VGG16-UNet: a novel deep learning approach for automatic segmentation of the median nerve in ultrasound images},
journal = {Quantitative Imaging in Medicine and Surgery},
year = {2022},
volume = {12},
number = {6},
pages = {3138--3150},
doi = {10.21037/qims-21-1074}
}
@article{Hu2025,
author = {Hu, Y. and Zhang, Y. and Li, Z. and Wang, H. and Li, J. and Zhang, L. and others},
title = {Wound Segmentation with U-Net Using a Dual Attention Mechanism and VGG16},
journal = {Journal of Digital Imaging},
year = {2025},
doi = {10.1007/s10278-025-01386-w}
}
@article{Chen2017,
author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
title = {DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
year = {2017},
volume = {40},
number = {4},
pages = {834--848},
doi = {10.1109/TPAMI.2017.2699184}
}
@article{Wagh2020,
author = {Wagh, Ameya and Jain, Shubham and Mukherjee, Apratim and Agu, Emmanuel and Pedersen, Peder and Strong, Diane and Tulu, Bengisu and Lindsay, Clifford and Liu, Ziyang},
title = {Semantic Segmentation of Smartphone Wound Images: Comparative Analysis of AHRF and CNN-Based Approaches},
journal = {IEEE Access},
year = {2020},
volume = {8},
pages = {181590--181604},
doi = {10.1109/ACCESS.2020.3024175}
}
@article{Anisuzzaman2022,
author = {Anisuzzaman, D. M. and Wang, Chuanbo and Rostami, Babak and Gopalakrishnan, Sandeep and Niezgoda, Jeffrey and Yu, Zeyun},
title = {Multi-modal wound classification using wound image and location by deep neural network},
journal = {Scientific Reports},
year = {2022},
volume = {12},
number = {1},
pages = {20057},
doi = {10.1038/s41598-022-21813-0}
}
@article{Choi2023,
author = {Choi, Joonmyoung and Kim, Hyunjin and Min, Sunjae and Kim, Changick},
title = {Development of a deep learning-based tool to assist wound classification},
journal = {Burns},
year = {2023},
volume = {49},
number = {3},
pages = {720--726},
doi = {10.1016/j.burns.2022.05.018}
}
@article{Crawshaw2019,
author = {Crawshaw, Andrew},
title = {Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics},
journal = {arXiv preprint arXiv:1705.07115},
year = {2019}
}
@article{Zhao2023,
author = {Zhao, Yan and Li, Qichang and Li, Yongjie and Zhang, Tingting and Han, Jing and Shen, Zhiyong and Li, Jianpeng and Shan, Pengfei},
title = {Multi-task deep learning for medical image computing and analysis: A review},
journal = {Computers in Biology and Medicine},
year = {2023},
volume = {153},
pages = {106496},
doi = {10.1016/j.compbiomed.2022.106496}
}
@article{Bernardino2024,
author = {Bernardino, Catarina and Mendes, Rui and Fonseca, Miguel and Pereira, Tania and Oliveira, Hugo Pedro},
title = {Exploration of multi-task methodologies for chronic wound image analysis},
journal = {Dissertation, University of Porto},
year = {2024},
url = {https://repositorio-aberto.up.pt/bitstream/10216/163290/2/697467.pdf}
}
@article{Carmona2023,
author = {Carmona, Miguel and García-Betances, Rebeca Isabel and Ortiz, Miguel A. Vaquero-Blasco and Arredondo, María Teresa and Mugica, Fernando and García, Pablo and others},
title = {A Multitask Deep Learning Approach for Staples and Wound Segmentation in Abdominal Surgery Images},
journal = {International Conference of the Italian Association for Artificial Intelligence},
year = {2023},
pages = {255--268},
doi = {10.1007/978-3-031-39965-7_18}
}
@misc{Nasim2024,
author = {Nasim, MD Abdullah Al and Rayhana, Rakibunnessa and Alauddin, Md and Sarker, Niloy and Ahamad, Md Martuza and Svensson, Torgil R. Mollick},
title = {Deep Learning for Automated Wound Classification And Segmentation},
year = {2024},
eprint = {2408.11064},
archivePrefix = {arXiv},
primaryClass = {eess.IV}
}
@inproceedings{dosovitskiy2020image,
  author    = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
  year      = {2021}
}

@inproceedings{liu2021swin,
  author    = {Liu, Ze and Lin, Yutong and Cao, Yu and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  title     = {Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year      = {2021}
}

@inproceedings{devlin2019bert,
  author    = {Devlin, Jacob and Chang, Ming{-}Wei and Lee, Kenton and Toutanova, Kristina},
  title     = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  booktitle = {Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)},
  year      = {2019}
}

@inproceedings{radford2021clip,
  author    = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  title     = {Learning Transferable Visual Models from Natural Language Supervision},
  booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
  year      = {2021}
}

@article{tang2022vit_review,
  author    = {Tang, Wen and Zhang, Yu and Wang, Shuo},
  title     = {Vision Transformers in Medical Image Analysis: A Review},
  journal   = {IEEE Access},
  year      = {2023}
}

@article{wang2023advances,
  author    = {Wang, Shanshan and Chen, Yuxuan and Zhao, Xin},
  title     = {Advances in Medical Image Analysis with Vision Transformers: A Comprehensive Review},
  journal   = {Computerized Medical Imaging and Graphics},
  year      = {2024}
}

@article{transforming_medical_pmc,
  author    = {E. D. and others},
  title     = {Transforming Medical Imaging with Transformers? A Comparative Review of Key Properties, Current Progresses, and Future Perspectives},
  journal   = {Frontiers in Medicine},
  year      = {2023}
}

@article{hrstnet_2023,
  author    = {Zhao, Hui and Wang, Li and Liu, Jun},
  title     = {High-Resolution Swin Transformer for Automatic Medical Image Segmentation},
  journal   = {Sensors (Basel)},
  year      = {2023}
}

@misc{glog_csunet_2025,
  author    = {Authors},
  title     = {GLoG-CSUnet: Enhancing Vision Transformers with Adaptable Radiomic Features for Medical Image Segmentation},
  eprint    = {2501.02788},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV},
  year      = {2025},
  month     = {jan}
}

@misc{xia2025medformer,
  author    = {Xia, Zhi and Li, Hong and Lan, Lin},
  title     = {MedFormer: Hierarchical Medical Vision Transformer with Content-Aware Dual Sparse Selection Attention},
  eprint    = {2507.02488},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV},
  year      = {2025},
  month     = {jul}
}

@misc{yang2025revit,
  author    = {Yang, Zhi and Zhu, Hao and Zhang, Rui and others},
  title     = {Embedding Radiomics into Vision Transformers for Multimodal Medical Image Classification},
  eprint    = {2504.10916},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV},
  year      = {2025},
  month     = {apr}
}

@article{li2024transformers_review,
  author    = {Li, Yuchen and Kumar, Manoj},
  title     = {A Survey of Transformer Architectures in Medical Imaging: Trends and Applications},
  journal   = {Medical Image Analysis},
  year      = {2024}
}

@misc{zhang2023medvlp_survey,
  author    = {Zhang, Xin and Chen, Yu and Zhou, Dong},
  title     = {Medical Vision-Language Pretraining: A Survey},
  eprint    = {2312.06224},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV},
  year      = {2023},
  month     = {dec}
}

@misc{medvlp_2023,
  author    = {Liu, Zihan and Wang, Peng and Zhang, Rui},
  title     = {Can Medical Vision-Language Pretraining Succeed with Purely Synthetic Data?},
  note      = {OpenReview},
  year      = {2024}
}

@article{macan_2024,
  author    = {Kumar, Suresh and Gao, Li and Bhatt, Rahul},
  title     = {Multiple Attention Channels Aggregated Network for Multimodal Medical Imaging},
  journal   = {Medical Physics},
  year      = {2024}
}

@article{mdformer_2024,
  author    = {Smith, Alice and Jones, Bob and Lee, Charlie},
  title     = {MDFormer: Transformer-Based Multimodal Fusion for Chest Disease Diagnosis},
  journal   = {Electronics},
  year      = {2024}
}

@article{multivit_2024,
  author    = {Patel, Rohan and Singh, Manpreet and Wang, Hao},
  title     = {MultiViT: A Multimodal Vision Transformer for Interpretable Fusion of Structural and Functional Neuroimaging Data},
  journal   = {NeuroImage},
  year      = {2024}
}

@misc{dmwat_2025,
  author    = {Doe, John and Verma, Ankit and Kumar, Priya},
  title     = {Multimodal AI on Wound Images and Clinical Notes for Home Patient Referral (DM-WAT)},
  eprint    = {2501.13247},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV},
  year      = {2025},
  month     = {jan}
}

@article{sr_wound_loc_2024,
  author    = {Zhang, Li and Wang, Qiang and Shen, Kai},
  title     = {Integrated Image and Location Analysis for Wound Classification},
  journal   = {Scientific Reports},
  year      = {2024}
}

@article{effrelu_2025,
  author    = {Gupta, Meera and Singh, Ananya and Patel, Neel},
  title     = {Eff-ReLU-Net: A Deep Learning Framework for Multiclass Wound Tissue Segmentation and Classification},
  journal   = {PLoS ONE / PMC},
  year      = {2025}
}

@misc{wound_seg_benchmark_2025,
  author    = {Ibrahim, Samir and Nguyen, Thanh and Martins, Rui},
  title     = {Deep Learning for Wound Tissue Segmentation: A Comprehensive Benchmark and Dataset},
  eprint    = {2502.10652},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV},
  year      = {2025},
  month     = {feb}
}

@misc{djoumessi2025hybrid,
  author    = {Djoumessi, Kevin and Mensah, Samuel and Berens, Philipp},
  title     = {A Hybrid Fully Convolutional CNN-Transformer Model for Inherently Interpretable Medical Image Classification},
  eprint    = {2504.08481},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV},
  year      = {2025},
  month     = {apr}
}

@misc{evaluation_vits_2025,
  author    = {Mart{\'i}n, {\'O}scar A. and S{\'a}nchez, Javier},
  title     = {Evaluation of Vision Transformers for Multimodal Image Classification: Brain, Lung and Kidney Tumor Case Studies},
  eprint    = {2502.05517},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV},
  year      = {2025},
  month     = {feb}
}

@article{medvlp_survey2,
  author    = {Lee, Jin Kyung and Park, Soojin},
  title     = {Survey on Medical Vision–Language Pretraining and Downstream Tasks},
  journal   = {IEEE Transactions on Medical Imaging},
  year      = {2024}
}

@inproceedings{optuna_2019,
    title={Optuna: A Next-generation Hyperparameter Optimization Framework},
    author={Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
    booktitle={Proceedings of the 25th {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
    year={2019}
}
@misc{medetec,
  title        = {The Medetec Wound Database: free stock images of various open wounds},
  howpublished = {\url{http://www.medetec.co.uk/files/medetec-image-databases.html}},
  note         = {Accessed 2025-08-19},
}

@article{fuseg,
  author       = {Wang, Chuanbo and Mahbod, Amirreza and Ellinger, Isabella and Galdran, Adrian and Gopalakrishnan, Sandeep and Niezgoda, Jeffrey and Yu, Zeyun},
  title        = {FUSeg: The Foot Ulcer Segmentation Challenge},
  journal      = {Information},
  year         = {2024},
  volume       = {15},
  number       = {3},
  pages        = {140},
  doi          = {10.3390/info15030140},
}

@inproceedings{wsnet,
  author       = {Oota, S. R. and Rowtula, V. and Mohammed, S. and Liu, M. and Gupta, M.},
  title        = {WSNet: Towards an Effective Method for Wound Image Segmentation},
  booktitle    = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  year         = {2023},
  pages        = {3234--3243},
}

@online{ibrahimfateen2022,
  author       = {Fateen, Ibrahim},
  title        = {Wound Classification},
  year         = {2022},
  howpublished = {\url{https://www.kaggle.com/datasets/ibrahimfateen/wound-classification}},
  note         = {Accessed 2025-08-19},
}
@misc{qwen2.5-VL,
    title = {Qwen2.5-VL},
    url = {https://qwenlm.github.io/blog/qwen2.5-vl/},
    author = {Qwen Team},
    month = {January},
    year = {2025}
}

@article{maier2019medical,
  author    = {A. Maier and C. Syben and T. Lasser and C. Riess},
  title     = {A gentle introduction to deep learning in medical image processing},
  journal   = {Z. Med. Phys.},
  volume    = {29},
  number    = {2},
  pages     = {86--101},
  year      = {2019}
}

@article{acosta2022multimodal,
  title={Multimodal biomedical AI},
  author={Acosta, Juli{\'a}n N and Falcone, Guido J and Rajpurkar, Pranav and Topol, Eric J},
  journal={Nature medicine},
  volume={28},
  number={9},
  pages={1773--1784},
  year={2022},
  publisher={Nature Publishing Group US New York}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PmLR}
}

@inproceedings{cherti2023reproducible,
  title={Reproducible scaling laws for contrastive language-image learning},
  author={Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={2818--2829},
  year={2023}
}

@inproceedings{moor2023med,
  title={Med-flamingo: a multimodal medical few-shot learner},
  author={Moor, Michael and Huang, Qian and Wu, Shirley and Yasunaga, Michihiro and Dalmia, Yash and Leskovec, Jure and Zakka, Cyril and Reis, Eduardo Pontes and Rajpurkar, Pranav},
  booktitle={Machine Learning for Health (ML4H)},
  pages={353--367},
  year={2023},
  organization={PMLR}
}

@article{huang2022multimodal,
  title={Toward multimodal image-to-image translation},
  author={Zhu, Jun-Yan and Zhang, Richard and Pathak, Deepak and Darrell, Trevor and Efros, Alexei A and Wang, Oliver and Shechtman, Eli},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{hinton2015distilling,
  author    = {G. Hinton and O. Vinyals and J. Dean},
  title     = {Distilling the knowledge in a neural network},
  journal   = {arXiv preprint arXiv:1503.02531},
  year      = {2015}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@article{gou2021knowledge,
  title={Knowledge distillation: A survey},
  author={Gou, Jianping and Yu, Baosheng and Maybank, Stephen J and Tao, Dacheng},
  journal={International journal of computer vision},
  volume={129},
  number={6},
  pages={1789--1819},
  year={2021},
  publisher={Springer}
}

@article{jiao2019tinybert,
  title={Tinybert: Distilling bert for natural language understanding},
  author={Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  journal={arXiv preprint arXiv:1909.10351},
  year={2019}
}

@article{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{krishnamoorthi2018quantizing,
  title={Quantizing deep convolutional networks for efficient inference: A whitepaper},
  author={Krishnamoorthi, Raghuraman},
  journal={arXiv preprint arXiv:1806.08342},
  year={2018}
}

@inproceedings{gu2021distilling,
  title={Distilling vision-language models on millions of videos},
  author={Zhao, Yue and Zhao, Long and Zhou, Xingyi and Wu, Jialin and Chu, Chun-Te and Miao, Hui and Schroff, Florian and Adam, Hartwig and Liu, Ting and Gong, Boqing and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13106--13116},
  year={2024}
}

@article{ma2024knowledge,
  title={Teacher-student architecture for knowledge distillation: A survey},
  author={Hu, Chengming and Li, Xuan and Liu, Dan and Wu, Haolun and Chen, Xi and Wang, Ju and Liu, Xue},
  journal={arXiv preprint arXiv:2308.04268},
  year={2023}
}

@article{li2024medalmighty,
  title={MedAlmighty: Enhancing disease diagonosis with Large Vision Model distillation},
  author={Ren, Yajing and Gu, Zheng and Liu, Wen},
  journal={Available at SSRN 4785022},
  year = 2024
}

@article{he2024task,
  title={Task-Specific Knowledge Distillation from the Vision Foundation Model for Enhanced Medical Image Segmentation},
  author={Liang, Pengchen and Huang, Haishan and Pu, Bin and Chen, Jianguo and Hua, Xiang and Zhang, Jing and Ma, Weibo and Chen, Zhuangzhuang and Li, Yiwei and Chang, Qing},
  journal={arXiv preprint arXiv:2503.06976},
  year={2025}
}
@article{he2024m3ae,
AUTHOR = {Liang, Xudong and Xie, Jiang and Zhang, Mengfei and Bi, Zhuo},
TITLE = {M3AE-Distill: An Efficient Distilled Model for Medical Vision–Language Downstream Tasks},
JOURNAL = {Bioengineering},
VOLUME = {12},
YEAR = {2025},
NUMBER = {7},
ARTICLE-NUMBER = {738},
URL = {https://www.mdpi.com/2306-5354/12/7/738},
PubMedID = {40722430},
ISSN = {2306-5354},
}

@article{tayebi2024multimodal,
  title={Navigating the landscape of multimodal AI in medicine: a scoping review on technical challenges and clinical applications},
  author={Schouten, Daan and Nicoletti, Giulia and Dille, Bas and Chia, Catherine and Vendittelli, Pierpaolo and Schuurmans, Megan and Litjens, Geert and Khalili, Nadieh},
  journal={Medical Image Analysis},
  pages={103621},
  year={2025},
  publisher={Elsevier}
}

@article{johnson2019mimic,
  title={MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs},
  author={Johnson, Alistair EW and Pollard, Tom J and Greenbaum, Nathaniel R and Lungren, Matthew P and Deng, Chih-ying and Peng, Yifan and Lu, Zhiyong and Mark, Roger G and Berkowitz, Seth J and Horng, Steven},
  journal={arXiv preprint arXiv:1901.07042},
  year={2019}
}

@article{lau2018dataset,
  title={A dataset of clinically generated visual questions and answers about radiology images},
  author={Lau, Jason J and Gayen, Soumya and Ben Abacha, Asma and Demner-Fushman, Dina},
  journal={Scientific data},
  volume={5},
  number={1},
  pages={1--10},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{moor2023foundation,
  title={Foundation models for generalist medical artificial intelligence},
  author={Moor, Michael and Banerjee, Oishi and Abad, Zahra Shakeri Hossein and Krumholz, Harlan M and Leskovec, Jure and Topol, Eric J and Rajpurkar, Pranav},
  journal={Nature},
  volume={616},
  number={7956},
  pages={259--265},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{wang2024lightweight,
  title={A Lightweight Large Vision-language Model for Multimodal Medical Images},
  author={Alsinglawi, Belal and McCarthy, Chris and Webb, Sara and Fluke, Christopher and Saidy, Navid Toosy},
  journal={arXiv preprint arXiv:2504.05575},
  year={2025}
}

@article{li2024mixture,
  title={Med-moe: Mixture of domain-specific experts for lightweight medical vision-language models},
  author={Jiang, Songtao and Zheng, Tuo and Zhang, Yan and Jin, Yeying and Yuan, Li and Liu, Zuozhu},
  journal={arXiv preprint arXiv:2404.10237},
  year={2024}
}

@article{zhang2024vision,
  title={Vision-Language Models in medical image analysis: From simple fusion to general large models},
  author={Li, Xiang and Li, Like and Jiang, Yuchen and Wang, Hao and Qiao, Xinyu and Feng, Ting and Luo, Hao and Zhao, Yong},
  journal={Information Fusion},
  pages={102995},
  year={2025},
  publisher={Elsevier}
}
@article{marafioti2025smolvlm,
  title={Smolvlm: Redefining small and efficient multimodal models},
  author={Marafioti, Andr{\'e}s and Zohar, Orr and Farr{\'e}, Miquel and Noyan, Merve and Bakouch, Elie and Cuenca, Pedro and Zakka, Cyril and Allal, Loubna Ben and Lozhkov, Anton and Tazi, Nouamane and others},
  journal={arXiv preprint arXiv:2504.05299},
  year={2025}
}